{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting torchtext\n  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n\u001b[?25hRequirement already satisfied: numpy in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.18.1)\nRequirement already satisfied: torch in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.4.0)\nRequirement already satisfied: tqdm in /Users/ryomisawa/.local/lib/python3.6/site-packages (from torchtext) (4.46.1)\nRequirement already satisfied: six in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.14.0)\nRequirement already satisfied: requests in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (2.23.0)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.91-cp36-cp36m-macosx_10_6_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 7.1 MB/s \n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (2020.4.5.1)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (2.9)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.91 torchtext-0.6.0\n"
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F \n",
    "import torchtext \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df_news = pd.read_csv('/Users/ryomisawa/Downloads/NewsAggregatorDataset/newsCorpora.csv', sep = '\\t', names=col_names)\n",
    "\n",
    "df_news = df_news[(df_news['PUBLISHER'] == 'Reuters') | (df_news['PUBLISHER'] ==  'Huffington Post')|(df_news['PUBLISHER'] == 'Businessweek') | (df_news['PUBLISHER'] == 'Contactmusic.com') |(df_news['PUBLISHER'] == 'Daily Mail')].sample(frac=1, random_state=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.drop('ID', axis=1, inplace=True)\n",
    "df_news.drop('URL', axis=1, inplace=True)\n",
    "df_news.drop('PUBLISHER', axis=1, inplace=True)\n",
    "df_news.drop('STORY', axis=1, inplace=True)\n",
    "df_news.drop('HOSTNAME', axis=1, inplace=True)\n",
    "df_news.drop('TIMESTAMP', axis=1, inplace=True)\n",
    "df_news.drop('index', axis=1, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_news_train, df_news_test = train_test_split(df_news, train_size=0.9)\n",
    "\n",
    "import os \n",
    "os.chdir('/Users/ryomisawa/nlp_tutorial')\n",
    "df_news_train.to_csv('news_train.csv')\n",
    "df_news_test.to_csv('news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_news['TITLE']\n",
    "df_y = df_news['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, y_train, y_valid = train_test_split(df_x, df_y, test_size=0.2)\n",
    "df_valid, df_test, y_valid, y_test = train_test_split(df_valid, y_valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "  text = re.sub('<br />', '', text)\n",
    "\n",
    "  for p in string.punctuation:\n",
    "      text = text.replace(p, ' ')\n",
    "  return text\n",
    "\n",
    "def tokenizer_punctuation(text):\n",
    "  return text.strip().split()\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "  text = preprocessing_text(text)\n",
    "  ret = tokenizer_punctuation(text)\n",
    "  return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "texts = tokenizer_with_preprocessing(df_train.to_string(index=False))\n",
    "texts = collections.Counter(texts).most_common()\n",
    "\n",
    "text_id = {}\n",
    "for i in range(len(texts)):\n",
    "    if texts[i][1] > 1:\n",
    "        text_id[texts[i][0]] = i+1\n",
    "    else:\n",
    "        text_id[texts[i][0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_nouse, y_train, y_nouse = train_test_split(df_x, df_y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2id(text):\n",
    "    ids = []\n",
    "    for word in text:\n",
    "        if word in text_id:\n",
    "            ids.append(text_id[word])\n",
    "        else:\n",
    "            ids.append(0)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_id = []\n",
    "for sentence in df_train:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_train_id.append(word2id(sentence))\n",
    "x_valid_id = []\n",
    "for sentence in df_valid:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_valid_id.append(word2id(sentence))\n",
    "#x_valid_oh = torch.tensor(x_valid_oh)\n",
    "x_test_id = []\n",
    "for sentence in df_test:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_test_id.append(word2id(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s) for s in x_train_id])\n",
    "for i in range(len(x_train_id)):\n",
    "    x_train_id[i] = x_train_id[i] + [0 for i in range(max_length - len(x_train_id[i]))]\n",
    "x_train_id = torch.tensor(x_train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s) for s in x_valid_id])\n",
    "for i in range(len(x_valid_id)):\n",
    "    x_valid_id[i] = x_valid_id[i] + [0 for i in range(max_length - len(x_valid_id[i]))]\n",
    "x_valid_id = torch.tensor(x_valid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s) for s in x_test_id])\n",
    "for i in range(len(x_test_id)):\n",
    "    x_test_id[i] = x_test_id[i] + [0 for i in range(max_length - len(x_test_id[i]))]\n",
    "x_test_id = torch.tensor(x_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq_train = torch.tensor([len(s) for s in x_train_id]) \n",
    "len_seq_valid = torch.tensor([len(s) for s in x_valid_id])\n",
    "len_seq_test = torch.tensor([len(s) for s in x_test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f764dd6ab469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_with_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx_train_oh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_valid_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_valid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(n, dtype)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \"\"\"\n\u001b[1;32m   2083\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meye\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/numpy/lib/twodim_base.py\u001b[0m in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype, order)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_length = max(text_id.values())\n",
    "x_train_oh = []\n",
    "for sentence in df_train:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_train_oh.append(np.identity(max_length+1)[word2id(sentence)])\n",
    "x_valid_oh = []\n",
    "for sentence in df_valid:\n",
    "    sentence = word2id(tokenizer_with_preprocessing(sentence))\n",
    "    x_valid_oh.append(np.identity(max_length+1)[word2id(sentence)])\n",
    "#x_valid_oh = torch.tensor(x_valid_oh)\n",
    "x_test_oh = []\n",
    "for sentence in df_test:\n",
    "    sentence = word2id(tokenizer_with_preprocessing(sentence))\n",
    "    x_test_oh.append(np.identity(max_length+1)[word2id(sentence)])\n",
    "#x_test_oh = torch.tensor(x_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = []\n",
    "t_valid = []\n",
    "t_test = []\n",
    "category = ['b', 'e', 't', 'm']\n",
    "number = [0, 1, 2, 3]\n",
    "n_labels = len(number)\n",
    "for text in y_train.replace(category, number):\n",
    "    t_train.append(text)\n",
    "t_train = torch.from_numpy(np.eye(n_labels)[t_train]).long()\n",
    "for text in y_valid.replace(category, number):\n",
    "    t_valid.append(text)\n",
    "t_valid = torch.from_numpy(np.eye(n_labels)[t_valid]).long()\n",
    "for text in y_test.replace(category, number):\n",
    "    t_test.append(text)\n",
    "t_test = torch.from_numpy(np.eye(n_labels)[t_test]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_matrix = nn.Parameter(torch.rand((vocab_size, emb_dim), dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.embedding(x, self.embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = nn.Parameter(torch.rand((word_num, emb_dim), dtype=torch.fl))\n",
    "h = F.embedding(x, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19"
     },
     "metadata": {},
     "execution_count": 399
    }
   ],
   "source": [
    "len(h[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19"
     },
     "metadata": {},
     "execution_count": 400
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "glorot = 6/(emb_dim + hid_dim*2)\n",
    "W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                       low=-np.sqrt(glorot),\n",
    "                       high=np.sqrt(glorot),\n",
    "                       size=(emb_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        \n",
    "        for i in range(len_seq_max):\n",
    "            state = torch.tanh(torch.matmul(torch.cat([state, x[i]], dim=1), W) + b)\n",
    "            output = torch.cat([output, state.unsqueeze(0)])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros((x[0].size()[0], hid_dim)).to(x.device)\n",
    "for i in range(len_seq_max):\n",
    "    state = torch.tanh(torch.matmul(torch.cat([state, x[i]], dim=1), W) + b)\n",
    "    output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = list(state.unsqueeze(0).size())\n",
    "size[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.empty(size, dtype=torch.float).to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros((x[0].size()[0], hid_dim)).to(x.device)\n",
    "for i in range(len_seq_max):\n",
    "    state = torch.tanh(torch.matmul(torch.cat([state, x[i]], dim=1), W) + b)\n",
    "    output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[0.1439, 0.1174, 0.8272,  ..., 0.8164, 0.2739, 0.2553],\n        [0.5425, 0.3298, 0.5337,  ..., 0.0745, 0.5213, 0.2803],\n        [0.4411, 0.0068, 0.5785,  ..., 0.4814, 0.6679, 0.9264],\n        ...,\n        [0.4956, 0.2459, 0.0520,  ..., 0.5274, 0.1826, 0.2962],\n        [0.5804, 0.5421, 0.3756,  ..., 0.1501, 0.0254, 0.0936],\n        [0.3551, 0.1195, 0.9951,  ..., 0.7573, 0.7799, 0.0235]],\n       grad_fn=<SelectBackward>)\ntensor([[0.7357, 0.7829, 0.4624,  ..., 0.5037, 0.3396, 0.8498],\n        [0.2736, 0.6062, 0.2357,  ..., 0.5083, 0.6638, 0.4232],\n        [0.1990, 0.8759, 0.7293,  ..., 0.5510, 0.5565, 0.1864],\n        ...,\n        [0.4211, 0.5575, 0.1450,  ..., 0.2999, 0.3643, 0.6648],\n        [0.1146, 0.7505, 0.1310,  ..., 0.6835, 0.8934, 0.7929],\n        [0.3097, 0.5993, 0.7906,  ..., 0.0590, 0.2146, 0.6339]],\n       grad_fn=<SelectBackward>)\ntensor([[0.3080, 0.1528, 0.3591,  ..., 0.4341, 0.2842, 0.6763],\n        [0.4535, 0.9851, 0.9125,  ..., 0.2180, 0.9269, 0.2111],\n        [0.4073, 0.4663, 0.1645,  ..., 0.8418, 0.7258, 0.0207],\n        ...,\n        [0.0467, 0.0909, 0.3106,  ..., 0.4710, 0.8896, 0.2187],\n        [0.9816, 0.0023, 0.3771,  ..., 0.0975, 0.3902, 0.1552],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4127, 0.0736, 0.9510,  ..., 0.9477, 0.5393, 0.6464],\n        [0.6762, 0.9340, 0.2311,  ..., 0.8375, 0.5995, 0.5784],\n        [0.9447, 0.0492, 0.6580,  ..., 0.6054, 0.8173, 0.3175],\n        ...,\n        [0.6600, 0.8840, 0.0240,  ..., 0.7588, 0.6389, 0.0650],\n        [0.2736, 0.6062, 0.2357,  ..., 0.5083, 0.6638, 0.4232],\n        [0.9989, 0.5081, 0.3608,  ..., 0.8416, 0.4589, 0.6379]],\n       grad_fn=<SelectBackward>)\ntensor([[0.5205, 0.7934, 0.8259,  ..., 0.7167, 0.3640, 0.1554],\n        [0.9948, 0.0771, 0.0237,  ..., 0.3110, 0.4571, 0.8805],\n        [0.5769, 0.8109, 0.6472,  ..., 0.7552, 0.1011, 0.2374],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.2742, 0.3969, 0.4053,  ..., 0.7066, 0.1157, 0.8030]],\n       grad_fn=<SelectBackward>)\ntensor([[0.8955, 0.6305, 0.2283,  ..., 0.1865, 0.4741, 0.7068],\n        [0.4562, 0.0468, 0.2917,  ..., 0.0763, 0.2849, 0.4207],\n        [0.4448, 0.4900, 0.6389,  ..., 0.0625, 0.5090, 0.4666],\n        ...,\n        [0.2535, 0.2432, 0.8892,  ..., 0.7152, 0.9709, 0.1164],\n        [0.3829, 0.9859, 0.0679,  ..., 0.0242, 0.0943, 0.0935],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.0693, 0.6674, 0.0471,  ..., 0.9009, 0.1243, 0.8707],\n        [0.2971, 0.3086, 0.2733,  ..., 0.7901, 0.0689, 0.5755],\n        ...,\n        [0.1619, 0.9831, 0.0236,  ..., 0.4662, 0.8951, 0.5371],\n        [0.0421, 0.4532, 0.2934,  ..., 0.1784, 0.5442, 0.4043],\n        [0.4134, 0.3379, 0.9932,  ..., 0.0593, 0.7067, 0.8931]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.5016, 0.7238, 0.3024,  ..., 0.1714, 0.8576, 0.7046],\n        [0.0700, 0.8899, 0.3586,  ..., 0.9066, 0.5155, 0.2848],\n        ...,\n        [0.3430, 0.1982, 0.7060,  ..., 0.6257, 0.0759, 0.2093],\n        [0.9933, 0.6927, 0.6217,  ..., 0.2578, 0.4750, 0.5511],\n        [0.8726, 0.3417, 0.1163,  ..., 0.9727, 0.1129, 0.6387]],\n       grad_fn=<SelectBackward>)\ntensor([[0.8898, 0.8766, 0.9005,  ..., 0.3632, 0.1642, 0.4270],\n        [0.7130, 0.2324, 0.2629,  ..., 0.9082, 0.1929, 0.9674],\n        [0.0976, 0.1584, 0.7277,  ..., 0.3670, 0.3633, 0.4793],\n        ...,\n        [0.2028, 0.2716, 0.3308,  ..., 0.7889, 0.2475, 0.3588],\n        [0.4134, 0.3379, 0.9932,  ..., 0.0593, 0.7067, 0.8931],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4454, 0.9792, 0.2076,  ..., 0.7337, 0.1114, 0.2542],\n        [0.8299, 0.4015, 0.1483,  ..., 0.5417, 0.6086, 0.6388],\n        [0.8031, 0.8018, 0.9556,  ..., 0.4899, 0.4822, 0.6274],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.9273, 0.6214, 0.3625,  ..., 0.8072, 0.2157, 0.1487],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.8632, 0.9961, 0.1676,  ..., 0.7852, 0.8497, 0.3061],\n        [0.9943, 0.4704, 0.6330,  ..., 0.7112, 0.5711, 0.5041],\n        [0.2604, 0.2223, 0.2532,  ..., 0.9976, 0.8808, 0.1190],\n        ...,\n        [0.4896, 0.5818, 0.3792,  ..., 0.7073, 0.9540, 0.5827],\n        [0.1341, 0.2555, 0.8277,  ..., 0.3266, 0.6132, 0.7651],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.0332, 0.7799, 0.6262,  ..., 0.5811, 0.9178, 0.3692],\n        [0.3576, 0.7681, 0.0773,  ..., 0.5890, 0.7110, 0.3931],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.8705, 0.7367, 0.2371,  ..., 0.1858, 0.7345, 0.0407],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.6935, 0.4095, 0.5183,  ..., 0.1229, 0.1836, 0.7478],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\ntensor([[0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        ...,\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248],\n        [0.4412, 0.4629, 0.3534,  ..., 0.0904, 0.1137, 0.5248]],\n       grad_fn=<SelectBackward>)\n"
    }
   ],
   "source": [
    "for i in range(len_seq_max):\n",
    "    print(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq_max = torch.max(len_seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros((hid_dim, x[0].size()[0],)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 1) cannot be concatenated",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-d6953c271a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_seq_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 1) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for s in x:\n",
    "    for i in range(len_seq_max):\n",
    "        state = torch.tanh(torch.matmul(torch.cat([state, s[i]], dim=1), W) + b)\n",
    "        output = torch.cat([output, state.unsqueeze(0)])\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        for i in range(len_seq_max):\n",
    "            state = self.function(state, x[i])\n",
    "            output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加\n",
    "        return output\n",
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(emb_dim, word_num)\n",
    "        self.rnn = RNN(emb_dim, hid_dim)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = self.emb(x)\n",
    "        h = self.rnn(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_id = x_train_id.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_id = torch.tensor(x_valid_id).long()\n",
    "x_test_id = torch.tensor(x_test_id).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.532, accuracy_train:0.425,loss_test:0.504, accuracy_test:0.395 \n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-8d2b70b819b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 誤差の逆伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# パラメータの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()\n",
    "    #n_train = 0\n",
    "    #acc_train = 0\n",
    "    net.zero_grad()\n",
    "\n",
    "    t = t_train.to(device)\n",
    "    x = x_train_id.to(device)\n",
    "    h = net(x, torch.max(len_seq_train), len_seq_train)\n",
    "    y = softmax(h).squeeze()\n",
    "\n",
    "    loss_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "    loss_train.backward()  # 誤差の逆伝播\n",
    "        \n",
    "    optimizer.step()  # パラメータの更新\n",
    "\n",
    "    acc_train = accuracy_score(t.argmax(axis=1), y.argmax(axis=1))\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    t = t_test.to(device)\n",
    "    x = x_test_id.to(device)\n",
    "    h = net(x, torch.max(len_seq_test), len_seq_test)\n",
    "    y = softmax(h).squeeze()\n",
    "\n",
    "    loss_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "    \n",
    "    acc_test = accuracy_score(t.argmax(axis=1), y.argmax(axis=1))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(loss_train, acc_train, loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-490-bb35c17f37d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0ml_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0ml_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.tensor([len(s) for s in x])\n",
    "        h = net(x, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    \n",
    "    net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.tensor([len(s) for s in x])\n",
    "        h = net(x, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/ryomisawa/Downloads/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1441     NML Slams Argentina's 'Brazen Step' to Pay Bon...\n11564    17 Times Zach Braff's 'Wish I Was Here' Refere...\n7339     Japan's Exports Decline in May on Weak US, Asi...\n10795    REFILE-China July official PMI rises to 51.7 f...\n3185     UPDATE 2-Bank of America ordered to pay $1.27 ...\n                               ...                        \n9253     UPDATE 2-US top court mostly upholds Obama bid...\n13265    Spotify's Most Popular Song Is Also The Most T...\n11983    Tila Tequila, Former Reality Star & Glamour Mo...\n8207     Jobless Claims in US Drop to Lowest Level Sinc...\n11277    Canceled And Renewed TV Shows For The 2014-'15...\nName: TITLE, Length: 6670, dtype: object"
     },
     "metadata": {},
     "execution_count": 322
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19"
     },
     "metadata": {},
     "execution_count": 403
    }
   ],
   "source": [
    "len(h[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19"
     },
     "metadata": {},
     "execution_count": 405
    }
   ],
   "source": [
    "len(h[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "\n",
    "train_embedding = []\n",
    "max_length_train = max([len(s.split()) for s in df_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df_train:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    train_embedding.append(x)\n",
    "train_embedding = torch.tensor(train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding =train_embedding.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "valid_embedding = []\n",
    "max_length_valid = max([len(s.split()) for s in df_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df_valid:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length_valid - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    valid_embedding.append(x)\n",
    "valid_embedding = torch.tensor(valid_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding =test_embedding.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "test_embedding = []\n",
    "max_length_test = max([len(s.split()) for s in df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df_test:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length_test - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    test_embedding.append(x)\n",
    "test_embedding = torch.tensor(test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        for s in x:\n",
    "            state = self.function(state, s)\n",
    "            output = torch.cat([output, state.unsqueeze(0)])  # 出力系列の追加\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet2(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = RNN(emb_dim, hid_dim)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, embedding, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        h = self.rnn(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.491, accuracy_train:0.434,loss_test:0.489, accuracy_test:0.463 \nloss_train:0.484, accuracy_train:0.472,loss_test:0.487, accuracy_test:0.403 \nloss_train:0.483, accuracy_train:0.485,loss_test:0.487, accuracy_test:0.425 \nloss_train:0.482, accuracy_train:0.488,loss_test:0.486, accuracy_test:0.507 \nloss_train:0.481, accuracy_train:0.498,loss_test:0.485, accuracy_test:0.503 \nloss_train:0.480, accuracy_train:0.497,loss_test:0.486, accuracy_test:0.503 \nloss_train:0.480, accuracy_train:0.501,loss_test:0.486, accuracy_test:0.500 \nloss_train:0.479, accuracy_train:0.503,loss_test:0.487, accuracy_test:0.506 \nloss_train:0.479, accuracy_train:0.507,loss_test:0.489, accuracy_test:0.470 \nloss_train:0.479, accuracy_train:0.508,loss_test:0.488, accuracy_test:0.501 \n"
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        \n",
    "        for i in range(len_seq_max):\n",
    "            state = torch.tanh(torch.matmul(torch.cat([state, x[i]], dim=1), W) + b)\n",
    "            output = torch.cat([output, state.unsqueeze(0)])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet2(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn_1 = RNN(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.rnn_2 = RNN(hid_dim, hid_dim, bidirectional=True)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, embedding, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        h = self.rnn_1(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [19 x 100], m2: [350 x 50] at ../aten/src/TH/generic/THTensorMath.cpp:41",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-522-2b0fadf1626e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mlen_seq_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_seq_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-521-b14e98e219fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, embedding, len_seq_max, len_seq, init_state)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen_seq\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-517-4d5b88dfbf41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, len_seq_max, init_state)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_seq_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [19 x 100], m2: [350 x 50] at ../aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet3(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(emb_dim, hid_dim, 4, batch_first=True)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self,x, embedding,  len_seq_max, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        if len_seq_max > 0:\n",
    "            h, _ = self.rnn(h[:, 0:len_seq_max, :], init_state)\n",
    "        else:\n",
    "            h, _ = self.rnn(h, init_state)\n",
    "        h = h.transpose(0, 1)\n",
    "        if len_seq is None:\n",
    "            h = h[len_seq - 1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.461, accuracy_train:0.602,loss_test:0.433, accuracy_test:0.651 \nloss_train:0.447, accuracy_train:0.629,loss_test:0.428, accuracy_test:0.651 \nloss_train:0.447, accuracy_train:0.632,loss_test:0.427, accuracy_test:0.651 \nloss_train:0.448, accuracy_train:0.639,loss_test:0.459, accuracy_test:0.652 \nloss_train:0.450, accuracy_train:0.633,loss_test:0.430, accuracy_test:0.652 \nloss_train:0.448, accuracy_train:0.636,loss_test:0.462, accuracy_test:0.651 \nloss_train:0.451, accuracy_train:0.634,loss_test:0.441, accuracy_test:0.650 \nloss_train:0.452, accuracy_train:0.622,loss_test:0.481, accuracy_test:0.651 \nloss_train:0.448, accuracy_train:0.625,loss_test:0.472, accuracy_test:0.651 \nloss_train:0.450, accuracy_train:0.627,loss_test:0.434, accuracy_test:0.651 \n"
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet3(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.ramdom.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, filter_shape, function=lambda x:x, stride=(1, 1), padding=1):\n",
    "        super().__init__()\n",
    "        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n",
    "        fan_out = filter_shape[0] * filter_shape[2] * filter_shape[3]\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "            low= -np.sqrt(6/fan_in),\n",
    "            high = np.sqrt(6/fan_in),\n",
    "            size=filter_shape\n",
    "        ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros((filter_shape[0]), dtype='float32')))\n",
    "        self.function = function\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    def forward(self, x):\n",
    "        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n",
    "        return self.function(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 50\n",
    "L = 4\n",
    "b_1 = nn.Parameter(torch.tensor(np.zeros((d_out), dtype='float32')))\n",
    "b_2 = nn.Parameter(torch.tensor(np.zeros((L), dtype='float32')))\n",
    "W_1 = torch.randn(d_out, 3*d, requires_grad=True)\n",
    "W_2 = torch.randn(L, d_out, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.ReLU()\n",
    "cs = []\n",
    "y_pred = []\n",
    "m = nn.Softmax(dim=0)\n",
    "for e in train_embedding:\n",
    "    c = []\n",
    "    p = []\n",
    "    for t in range(1, len(e)-1):\n",
    "        concat = torch.cat((e[t-1], e[t], e[t+1]), 0)\n",
    "        p.append(f(torch.matmul(W_1, concat) + b_1))\n",
    "    for i in range(d_out):\n",
    "        c.append(max([s[i].item() for s in p]))\n",
    "    c = torch.tensor(c).float()\n",
    "    y = m(torch.matmul(W_2, c) + b_2)\n",
    "    y_pred.append(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train = -torch.mean(t_train*torch_log(y_pred_train) + (1-t_train)*torch_log(1-y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1.2746e-37, 0.0000e+00, 3.2418e-29, 1.0000e+00],\n       grad_fn=<SoftmaxBackward>)"
     },
     "metadata": {},
     "execution_count": 676
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-675-235eb7130143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "l_train.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:10.524, acc_train:0.068, loss_test:10.441, acc_test:0.000\nloss_train:10.512, acc_train:0.067, loss_test:10.432, acc_test:0.000\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-715-b3cec2085a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-715-b3cec2085a24>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "optimizer = optim.Adam([b_1, b_2, W_1, W_2], lr=0.001)\n",
    "f = nn.ReLU()\n",
    "y_pred_train = []\n",
    "y_pred_test = []\n",
    "\n",
    "m = nn.Softmax(dim=0)\n",
    "for epoch in range(epochs):\n",
    "    l_train = 0\n",
    "    acc_train = 0\n",
    "    for i in range(len(train_embedding)):\n",
    "        e = train_embedding[i]\n",
    "        c = []\n",
    "        p = []\n",
    "        for t in range(1, len(e)-1):\n",
    "            concat = torch.cat((e[t-1], e[t], e[t+1]), 0)\n",
    "            p.append(f(torch.matmul(W_1, concat) + b_1))\n",
    "        for l in range(d_out):\n",
    "            c.append(max([s[l].item() for s in p]))\n",
    "        c = torch.tensor(c).float()\n",
    "        y = m(torch.matmul(W_2, c) + b_2)\n",
    "        l_train += -(t_train[i]*torch_log(y) + (1-t_train[i])*torch_log(1-y))\n",
    "        if t_train[i].argmax(axis=0).item() == y.argmax(axis=0).item():\n",
    "            acc_train += 1\n",
    "    l_train = torch.mean(l_train/len(train_embedding))\n",
    "    optimizer.zero_grad()\n",
    "    l_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    accuracy_train = acc_train/len(train_embedding)\n",
    "\n",
    "    l_test = 0\n",
    "    acc_test = 0\n",
    "    \n",
    "    for i in range(len(test_embedding)):\n",
    "        e = test_embedding[i]\n",
    "        c = []\n",
    "        p = []\n",
    "        for t in range(1, len(e)-1):\n",
    "            concat = torch.cat((e[t-1], e[t], e[t+1]), 0)\n",
    "            p.append(f(torch.matmul(W_1, concat) + b_1))\n",
    "        for l in range(d_out):\n",
    "            c.append(max([s[l].item() for s in p]))\n",
    "        c = torch.tensor(c).float()\n",
    "        y = m(torch.matmul(W_2, c) + b_2)\n",
    "        l_test += -(t_test[i]*torch_log(y) + (1-t_test[i])*torch_log(1-y))\n",
    "        if t_test[i].argmax(axis=0).item() == y.argmax(axis=0).item():\n",
    "            acc_train +=1\n",
    "    l_test = torch.mean(l_test/len(test_embedding))\n",
    "    acc_test = acc_test/len(test_embedding)\n",
    "\n",
    "    print('loss_train:{:.3f}, acc_train:{:.3f}, loss_test:{:.3f}, acc_test:{:.3f}'.format(l_train.item(), accuracy_train ,l_test.item(), acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "768"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "config_file = \"/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/weights/bert_config.json\"\n",
    "\n",
    "json_file = open(config_file, 'r')\n",
    "config = json.load(json_file)\n",
    "\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings,config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        self.output = BertOutput(config)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        \n",
    "        if attention_show_flg == True:\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "    \n",
    "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
    "        \n",
    "        if attention_show_flg ==True:\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, \n",
    "                                                         attention_mask,\n",
    "                                                         attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask,\n",
    "                                        attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output \n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "\n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads)\n",
    "        \n",
    "        self.all_head_size = self.num_attention_heads *  \\\n",
    "          self.attention_head_size\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "    \n",
    "    def transpose_for_scores(self, x):\n",
    "\n",
    "        new_x_shape = x.size()[\n",
    "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[\n",
    "            :-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        if attention_show_flg == True:\n",
    "            return context_layer, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            return context_layer\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "\n",
    "        self.intermediate_act_fn = gelu\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "class BertOutput(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        self.layer = nn.ModuleList([BertLayer(config)\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask,\n",
    "                output_all_encoded_layers=True, attention_show_flg=False):\n",
    "               \n",
    "                all_encoder_layers = []\n",
    "\n",
    "                for layer_module in self.layer:\n",
    "\n",
    "                    if attention_show_flg == True:\n",
    "                        hidden_states, attention_probs = layer_module(\n",
    "                            hidden_states, attention_mask, attention_show_flg)\n",
    "\n",
    "                    elif attention_show_flg == False:\n",
    "                        hidden_states = layer_module(\n",
    "                            hidden_states, attention_mask, attention_show_flg)\n",
    "                    \n",
    "                    if output_all_encoded_layers:\n",
    "                        all_encoder_layers.append(hidden_states)\n",
    "                \n",
    "                if not output_all_encoded_layers:\n",
    "                    all_encoder_layers.append(hidden_states)\n",
    "                \n",
    "                if attention_show_flg == True:\n",
    "                    return all_encoder_layers, attention_probs\n",
    "                elif attention_show_flg == False:\n",
    "                    return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "入力の単語ID列のテンソルサイズ： torch.Size([2, 5])\n入力のマスクのテンソルサイズ： torch.Size([2, 5])\n入力の文章IDのテンソルサイズ： torch.Size([2, 5])\n拡張したマスクのテンソルサイズ： torch.Size([2, 1, 1, 5])\nBertEmbeddingsの出力テンソルサイズ： torch.Size([2, 5, 768])\nBertEncoderの最終層の出力テンソルサイズ： torch.Size([2, 5, 768])\nBertPoolerの出力テンソルサイズ： torch.Size([2, 768])\n"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
    "\n",
    "# マスク\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print(\"入力のマスクのテンソルサイズ：\", attention_mask.shape)\n",
    "\n",
    "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
    "\n",
    "\n",
    "# BERTの各モジュールを用意\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# マスクの変形　[batch_size, 1, 1, seq_length]にする\n",
    "# Attentionをかけない部分はマイナス無限にしたいので、代わりに-10000をかけ算しています\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print(\"拡張したマスクのテンソルサイズ：\", extended_attention_mask.shape)\n",
    "\n",
    "# 順伝搬する\n",
    "out1 = embeddings(input_ids)\n",
    "print(\"BertEmbeddingsの出力テンソルサイズ：\", out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2は、[minibatch, seq_length, embedding_dim]が12個のリスト\n",
    "print(\"BertEncoderの最終層の出力テンソルサイズ：\", out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1])  # out2は12層の特徴量のリストになっているので一番最後を使用\n",
    "print(\"BertPoolerの出力テンソルサイズ：\", out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.embeddings = BertEmbedding(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None,output_all_encoded_layers=True, attention_show_flg=False):\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones_like(input_ids)\n",
    "                \n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "                extended_attention_mask = extended_attention_mask.to(\n",
    "                    dtype=torch.float32)\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * - 10000.0\n",
    "\n",
    "                embedding_output = self.embeddings(input_ids)\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                                   extended_attention_mask,\n",
    "                                                                   output_all_encoded_layers,\n",
    "                                                                   attention_show_flg)\n",
    "                elif attention_show_flg == False:\n",
    "                    encoded_layers = self.encoder(embedding_output,\n",
    "                                                  extended_attention_mask,\n",
    "                                                  output_all_encoded_layers,\n",
    "                                                  attention_show_flg)\n",
    "                \n",
    "                pooled_output = self.pooler(encoded_layers[-1])\n",
    "\n",
    "                if not output_all_encoded_layers:\n",
    "                    encoded_layers = encoded_layers[-1]\n",
    "                \n",
    "                if attention_show_flg == True:\n",
    "                    return encoded_layers, pooled_output, attention_probs\n",
    "                elif attention_show_flg == False:\n",
    "                    return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "encoded_layersのテンソルサイズ： torch.Size([2, 5, 768])\npooled_outputのテンソルサイズ： torch.Size([2, 768])\nattention_probsのテンソルサイズ： torch.Size([2, 12, 5, 5])\n"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERTモデルを作る\n",
    "net = BertModel(config)\n",
    "\n",
    "# 順伝搬させる\n",
    "encoded_layers, pooled_output, attention_probs= net(input_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "print(\"encoded_layersのテンソルサイズ：\", encoded_layers.shape)\n",
    "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
    "print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    ids_to_tokens = collections.OrderedDict()\n",
    "    index = 0\n",
    "\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "\n",
    "            vocab[token] = index\n",
    "            ids_to_tokens[index] = token\n",
    "            index += 1\n",
    "    return vocab, ids_to_tokens\n",
    "\n",
    "vocab_file = '/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt'\n",
    "vocab, ids_to_tokens = load_vocab(vocab_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert')\n",
    "\n",
    "from utils.tokenizer import BasicTokenizer, WordpieceTokenizer\n",
    "\n",
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
    "\n",
    "        never_split = ('[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]')\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_cae=do_lower_case,\n",
    "                                              never_split=never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    for p in string.punctuation:\n",
    "        if (p == '.') or (p == ','):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, ' ')\n",
    "    \n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    return text\n",
    "\n",
    "tokenizer_bert = BertTokenizer(\n",
    "    vocab_file='/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt', do_lower_case=True)\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "bert.embeddings.word_embeddings.weight→embeddings.word_embeddings.weight\nbert.embeddings.position_embeddings.weight→embeddings.position_embeddings.weight\nbert.embeddings.token_type_embeddings.weight→embeddings.LayerNorm.gamma\nbert.embeddings.LayerNorm.gamma→embeddings.LayerNorm.beta\nbert.embeddings.LayerNorm.beta→encoder.layer.0.attention.selfattn.query.weight\nbert.encoder.layer.0.attention.self.query.weight→encoder.layer.0.attention.selfattn.query.bias\nbert.encoder.layer.0.attention.self.query.bias→encoder.layer.0.attention.selfattn.key.weight\nbert.encoder.layer.0.attention.self.key.weight→encoder.layer.0.attention.selfattn.key.bias\nbert.encoder.layer.0.attention.self.key.bias→encoder.layer.0.attention.selfattn.value.weight\nbert.encoder.layer.0.attention.self.value.weight→encoder.layer.0.attention.selfattn.value.bias\nbert.encoder.layer.0.attention.self.value.bias→encoder.layer.0.attention.output.dense.weight\nbert.encoder.layer.0.attention.output.dense.weight→encoder.layer.0.attention.output.dense.bias\nbert.encoder.layer.0.attention.output.dense.bias→encoder.layer.0.attention.output.LayerNorm.gamma\nbert.encoder.layer.0.attention.output.LayerNorm.gamma→encoder.layer.0.attention.output.LayerNorm.beta\nbert.encoder.layer.0.attention.output.LayerNorm.beta→encoder.layer.0.intermediate.dense.weight\nbert.encoder.layer.0.intermediate.dense.weight→encoder.layer.0.intermediate.dense.bias\nbert.encoder.layer.0.intermediate.dense.bias→encoder.layer.0.output.dense.weight\nbert.encoder.layer.0.output.dense.weight→encoder.layer.0.output.dense.bias\nbert.encoder.layer.0.output.dense.bias→encoder.layer.0.output.LayerNorm.gamma\nbert.encoder.layer.0.output.LayerNorm.gamma→encoder.layer.0.output.LayerNorm.beta\nbert.encoder.layer.0.output.LayerNorm.beta→encoder.layer.1.attention.selfattn.query.weight\nbert.encoder.layer.1.attention.self.query.weight→encoder.layer.1.attention.selfattn.query.bias\nbert.encoder.layer.1.attention.self.query.bias→encoder.layer.1.attention.selfattn.key.weight\nbert.encoder.layer.1.attention.self.key.weight→encoder.layer.1.attention.selfattn.key.bias\nbert.encoder.layer.1.attention.self.key.bias→encoder.layer.1.attention.selfattn.value.weight\nbert.encoder.layer.1.attention.self.value.weight→encoder.layer.1.attention.selfattn.value.bias\nbert.encoder.layer.1.attention.self.value.bias→encoder.layer.1.attention.output.dense.weight\nbert.encoder.layer.1.attention.output.dense.weight→encoder.layer.1.attention.output.dense.bias\nbert.encoder.layer.1.attention.output.dense.bias→encoder.layer.1.attention.output.LayerNorm.gamma\nbert.encoder.layer.1.attention.output.LayerNorm.gamma→encoder.layer.1.attention.output.LayerNorm.beta\nbert.encoder.layer.1.attention.output.LayerNorm.beta→encoder.layer.1.intermediate.dense.weight\nbert.encoder.layer.1.intermediate.dense.weight→encoder.layer.1.intermediate.dense.bias\nbert.encoder.layer.1.intermediate.dense.bias→encoder.layer.1.output.dense.weight\nbert.encoder.layer.1.output.dense.weight→encoder.layer.1.output.dense.bias\nbert.encoder.layer.1.output.dense.bias→encoder.layer.1.output.LayerNorm.gamma\nbert.encoder.layer.1.output.LayerNorm.gamma→encoder.layer.1.output.LayerNorm.beta\nbert.encoder.layer.1.output.LayerNorm.beta→encoder.layer.2.attention.selfattn.query.weight\nbert.encoder.layer.2.attention.self.query.weight→encoder.layer.2.attention.selfattn.query.bias\nbert.encoder.layer.2.attention.self.query.bias→encoder.layer.2.attention.selfattn.key.weight\nbert.encoder.layer.2.attention.self.key.weight→encoder.layer.2.attention.selfattn.key.bias\nbert.encoder.layer.2.attention.self.key.bias→encoder.layer.2.attention.selfattn.value.weight\nbert.encoder.layer.2.attention.self.value.weight→encoder.layer.2.attention.selfattn.value.bias\nbert.encoder.layer.2.attention.self.value.bias→encoder.layer.2.attention.output.dense.weight\nbert.encoder.layer.2.attention.output.dense.weight→encoder.layer.2.attention.output.dense.bias\nbert.encoder.layer.2.attention.output.dense.bias→encoder.layer.2.attention.output.LayerNorm.gamma\nbert.encoder.layer.2.attention.output.LayerNorm.gamma→encoder.layer.2.attention.output.LayerNorm.beta\nbert.encoder.layer.2.attention.output.LayerNorm.beta→encoder.layer.2.intermediate.dense.weight\nbert.encoder.layer.2.intermediate.dense.weight→encoder.layer.2.intermediate.dense.bias\nbert.encoder.layer.2.intermediate.dense.bias→encoder.layer.2.output.dense.weight\nbert.encoder.layer.2.output.dense.weight→encoder.layer.2.output.dense.bias\nbert.encoder.layer.2.output.dense.bias→encoder.layer.2.output.LayerNorm.gamma\nbert.encoder.layer.2.output.LayerNorm.gamma→encoder.layer.2.output.LayerNorm.beta\nbert.encoder.layer.2.output.LayerNorm.beta→encoder.layer.3.attention.selfattn.query.weight\nbert.encoder.layer.3.attention.self.query.weight→encoder.layer.3.attention.selfattn.query.bias\nbert.encoder.layer.3.attention.self.query.bias→encoder.layer.3.attention.selfattn.key.weight\nbert.encoder.layer.3.attention.self.key.weight→encoder.layer.3.attention.selfattn.key.bias\nbert.encoder.layer.3.attention.self.key.bias→encoder.layer.3.attention.selfattn.value.weight\nbert.encoder.layer.3.attention.self.value.weight→encoder.layer.3.attention.selfattn.value.bias\nbert.encoder.layer.3.attention.self.value.bias→encoder.layer.3.attention.output.dense.weight\nbert.encoder.layer.3.attention.output.dense.weight→encoder.layer.3.attention.output.dense.bias\nbert.encoder.layer.3.attention.output.dense.bias→encoder.layer.3.attention.output.LayerNorm.gamma\nbert.encoder.layer.3.attention.output.LayerNorm.gamma→encoder.layer.3.attention.output.LayerNorm.beta\nbert.encoder.layer.3.attention.output.LayerNorm.beta→encoder.layer.3.intermediate.dense.weight\nbert.encoder.layer.3.intermediate.dense.weight→encoder.layer.3.intermediate.dense.bias\nbert.encoder.layer.3.intermediate.dense.bias→encoder.layer.3.output.dense.weight\nbert.encoder.layer.3.output.dense.weight→encoder.layer.3.output.dense.bias\nbert.encoder.layer.3.output.dense.bias→encoder.layer.3.output.LayerNorm.gamma\nbert.encoder.layer.3.output.LayerNorm.gamma→encoder.layer.3.output.LayerNorm.beta\nbert.encoder.layer.3.output.LayerNorm.beta→encoder.layer.4.attention.selfattn.query.weight\nbert.encoder.layer.4.attention.self.query.weight→encoder.layer.4.attention.selfattn.query.bias\nbert.encoder.layer.4.attention.self.query.bias→encoder.layer.4.attention.selfattn.key.weight\nbert.encoder.layer.4.attention.self.key.weight→encoder.layer.4.attention.selfattn.key.bias\nbert.encoder.layer.4.attention.self.key.bias→encoder.layer.4.attention.selfattn.value.weight\nbert.encoder.layer.4.attention.self.value.weight→encoder.layer.4.attention.selfattn.value.bias\nbert.encoder.layer.4.attention.self.value.bias→encoder.layer.4.attention.output.dense.weight\nbert.encoder.layer.4.attention.output.dense.weight→encoder.layer.4.attention.output.dense.bias\nbert.encoder.layer.4.attention.output.dense.bias→encoder.layer.4.attention.output.LayerNorm.gamma\nbert.encoder.layer.4.attention.output.LayerNorm.gamma→encoder.layer.4.attention.output.LayerNorm.beta\nbert.encoder.layer.4.attention.output.LayerNorm.beta→encoder.layer.4.intermediate.dense.weight\nbert.encoder.layer.4.intermediate.dense.weight→encoder.layer.4.intermediate.dense.bias\nbert.encoder.layer.4.intermediate.dense.bias→encoder.layer.4.output.dense.weight\nbert.encoder.layer.4.output.dense.weight→encoder.layer.4.output.dense.bias\nbert.encoder.layer.4.output.dense.bias→encoder.layer.4.output.LayerNorm.gamma\nbert.encoder.layer.4.output.LayerNorm.gamma→encoder.layer.4.output.LayerNorm.beta\nbert.encoder.layer.4.output.LayerNorm.beta→encoder.layer.5.attention.selfattn.query.weight\nbert.encoder.layer.5.attention.self.query.weight→encoder.layer.5.attention.selfattn.query.bias\nbert.encoder.layer.5.attention.self.query.bias→encoder.layer.5.attention.selfattn.key.weight\nbert.encoder.layer.5.attention.self.key.weight→encoder.layer.5.attention.selfattn.key.bias\nbert.encoder.layer.5.attention.self.key.bias→encoder.layer.5.attention.selfattn.value.weight\nbert.encoder.layer.5.attention.self.value.weight→encoder.layer.5.attention.selfattn.value.bias\nbert.encoder.layer.5.attention.self.value.bias→encoder.layer.5.attention.output.dense.weight\nbert.encoder.layer.5.attention.output.dense.weight→encoder.layer.5.attention.output.dense.bias\nbert.encoder.layer.5.attention.output.dense.bias→encoder.layer.5.attention.output.LayerNorm.gamma\nbert.encoder.layer.5.attention.output.LayerNorm.gamma→encoder.layer.5.attention.output.LayerNorm.beta\nbert.encoder.layer.5.attention.output.LayerNorm.beta→encoder.layer.5.intermediate.dense.weight\nbert.encoder.layer.5.intermediate.dense.weight→encoder.layer.5.intermediate.dense.bias\nbert.encoder.layer.5.intermediate.dense.bias→encoder.layer.5.output.dense.weight\nbert.encoder.layer.5.output.dense.weight→encoder.layer.5.output.dense.bias\nbert.encoder.layer.5.output.dense.bias→encoder.layer.5.output.LayerNorm.gamma\nbert.encoder.layer.5.output.LayerNorm.gamma→encoder.layer.5.output.LayerNorm.beta\nbert.encoder.layer.5.output.LayerNorm.beta→encoder.layer.6.attention.selfattn.query.weight\nbert.encoder.layer.6.attention.self.query.weight→encoder.layer.6.attention.selfattn.query.bias\nbert.encoder.layer.6.attention.self.query.bias→encoder.layer.6.attention.selfattn.key.weight\nbert.encoder.layer.6.attention.self.key.weight→encoder.layer.6.attention.selfattn.key.bias\nbert.encoder.layer.6.attention.self.key.bias→encoder.layer.6.attention.selfattn.value.weight\nbert.encoder.layer.6.attention.self.value.weight→encoder.layer.6.attention.selfattn.value.bias\nbert.encoder.layer.6.attention.self.value.bias→encoder.layer.6.attention.output.dense.weight\nbert.encoder.layer.6.attention.output.dense.weight→encoder.layer.6.attention.output.dense.bias\nbert.encoder.layer.6.attention.output.dense.bias→encoder.layer.6.attention.output.LayerNorm.gamma\nbert.encoder.layer.6.attention.output.LayerNorm.gamma→encoder.layer.6.attention.output.LayerNorm.beta\nbert.encoder.layer.6.attention.output.LayerNorm.beta→encoder.layer.6.intermediate.dense.weight\nbert.encoder.layer.6.intermediate.dense.weight→encoder.layer.6.intermediate.dense.bias\nbert.encoder.layer.6.intermediate.dense.bias→encoder.layer.6.output.dense.weight\nbert.encoder.layer.6.output.dense.weight→encoder.layer.6.output.dense.bias\nbert.encoder.layer.6.output.dense.bias→encoder.layer.6.output.LayerNorm.gamma\nbert.encoder.layer.6.output.LayerNorm.gamma→encoder.layer.6.output.LayerNorm.beta\nbert.encoder.layer.6.output.LayerNorm.beta→encoder.layer.7.attention.selfattn.query.weight\nbert.encoder.layer.7.attention.self.query.weight→encoder.layer.7.attention.selfattn.query.bias\nbert.encoder.layer.7.attention.self.query.bias→encoder.layer.7.attention.selfattn.key.weight\nbert.encoder.layer.7.attention.self.key.weight→encoder.layer.7.attention.selfattn.key.bias\nbert.encoder.layer.7.attention.self.key.bias→encoder.layer.7.attention.selfattn.value.weight\nbert.encoder.layer.7.attention.self.value.weight→encoder.layer.7.attention.selfattn.value.bias\nbert.encoder.layer.7.attention.self.value.bias→encoder.layer.7.attention.output.dense.weight\nbert.encoder.layer.7.attention.output.dense.weight→encoder.layer.7.attention.output.dense.bias\nbert.encoder.layer.7.attention.output.dense.bias→encoder.layer.7.attention.output.LayerNorm.gamma\nbert.encoder.layer.7.attention.output.LayerNorm.gamma→encoder.layer.7.attention.output.LayerNorm.beta\nbert.encoder.layer.7.attention.output.LayerNorm.beta→encoder.layer.7.intermediate.dense.weight\nbert.encoder.layer.7.intermediate.dense.weight→encoder.layer.7.intermediate.dense.bias\nbert.encoder.layer.7.intermediate.dense.bias→encoder.layer.7.output.dense.weight\nbert.encoder.layer.7.output.dense.weight→encoder.layer.7.output.dense.bias\nbert.encoder.layer.7.output.dense.bias→encoder.layer.7.output.LayerNorm.gamma\nbert.encoder.layer.7.output.LayerNorm.gamma→encoder.layer.7.output.LayerNorm.beta\nbert.encoder.layer.7.output.LayerNorm.beta→encoder.layer.8.attention.selfattn.query.weight\nbert.encoder.layer.8.attention.self.query.weight→encoder.layer.8.attention.selfattn.query.bias\nbert.encoder.layer.8.attention.self.query.bias→encoder.layer.8.attention.selfattn.key.weight\nbert.encoder.layer.8.attention.self.key.weight→encoder.layer.8.attention.selfattn.key.bias\nbert.encoder.layer.8.attention.self.key.bias→encoder.layer.8.attention.selfattn.value.weight\nbert.encoder.layer.8.attention.self.value.weight→encoder.layer.8.attention.selfattn.value.bias\nbert.encoder.layer.8.attention.self.value.bias→encoder.layer.8.attention.output.dense.weight\nbert.encoder.layer.8.attention.output.dense.weight→encoder.layer.8.attention.output.dense.bias\nbert.encoder.layer.8.attention.output.dense.bias→encoder.layer.8.attention.output.LayerNorm.gamma\nbert.encoder.layer.8.attention.output.LayerNorm.gamma→encoder.layer.8.attention.output.LayerNorm.beta\nbert.encoder.layer.8.attention.output.LayerNorm.beta→encoder.layer.8.intermediate.dense.weight\nbert.encoder.layer.8.intermediate.dense.weight→encoder.layer.8.intermediate.dense.bias\nbert.encoder.layer.8.intermediate.dense.bias→encoder.layer.8.output.dense.weight\nbert.encoder.layer.8.output.dense.weight→encoder.layer.8.output.dense.bias\nbert.encoder.layer.8.output.dense.bias→encoder.layer.8.output.LayerNorm.gamma\nbert.encoder.layer.8.output.LayerNorm.gamma→encoder.layer.8.output.LayerNorm.beta\nbert.encoder.layer.8.output.LayerNorm.beta→encoder.layer.9.attention.selfattn.query.weight\nbert.encoder.layer.9.attention.self.query.weight→encoder.layer.9.attention.selfattn.query.bias\nbert.encoder.layer.9.attention.self.query.bias→encoder.layer.9.attention.selfattn.key.weight\nbert.encoder.layer.9.attention.self.key.weight→encoder.layer.9.attention.selfattn.key.bias\nbert.encoder.layer.9.attention.self.key.bias→encoder.layer.9.attention.selfattn.value.weight\nbert.encoder.layer.9.attention.self.value.weight→encoder.layer.9.attention.selfattn.value.bias\nbert.encoder.layer.9.attention.self.value.bias→encoder.layer.9.attention.output.dense.weight\nbert.encoder.layer.9.attention.output.dense.weight→encoder.layer.9.attention.output.dense.bias\nbert.encoder.layer.9.attention.output.dense.bias→encoder.layer.9.attention.output.LayerNorm.gamma\nbert.encoder.layer.9.attention.output.LayerNorm.gamma→encoder.layer.9.attention.output.LayerNorm.beta\nbert.encoder.layer.9.attention.output.LayerNorm.beta→encoder.layer.9.intermediate.dense.weight\nbert.encoder.layer.9.intermediate.dense.weight→encoder.layer.9.intermediate.dense.bias\nbert.encoder.layer.9.intermediate.dense.bias→encoder.layer.9.output.dense.weight\nbert.encoder.layer.9.output.dense.weight→encoder.layer.9.output.dense.bias\nbert.encoder.layer.9.output.dense.bias→encoder.layer.9.output.LayerNorm.gamma\nbert.encoder.layer.9.output.LayerNorm.gamma→encoder.layer.9.output.LayerNorm.beta\nbert.encoder.layer.9.output.LayerNorm.beta→encoder.layer.10.attention.selfattn.query.weight\nbert.encoder.layer.10.attention.self.query.weight→encoder.layer.10.attention.selfattn.query.bias\nbert.encoder.layer.10.attention.self.query.bias→encoder.layer.10.attention.selfattn.key.weight\nbert.encoder.layer.10.attention.self.key.weight→encoder.layer.10.attention.selfattn.key.bias\nbert.encoder.layer.10.attention.self.key.bias→encoder.layer.10.attention.selfattn.value.weight\nbert.encoder.layer.10.attention.self.value.weight→encoder.layer.10.attention.selfattn.value.bias\nbert.encoder.layer.10.attention.self.value.bias→encoder.layer.10.attention.output.dense.weight\nbert.encoder.layer.10.attention.output.dense.weight→encoder.layer.10.attention.output.dense.bias\nbert.encoder.layer.10.attention.output.dense.bias→encoder.layer.10.attention.output.LayerNorm.gamma\nbert.encoder.layer.10.attention.output.LayerNorm.gamma→encoder.layer.10.attention.output.LayerNorm.beta\nbert.encoder.layer.10.attention.output.LayerNorm.beta→encoder.layer.10.intermediate.dense.weight\nbert.encoder.layer.10.intermediate.dense.weight→encoder.layer.10.intermediate.dense.bias\nbert.encoder.layer.10.intermediate.dense.bias→encoder.layer.10.output.dense.weight\nbert.encoder.layer.10.output.dense.weight→encoder.layer.10.output.dense.bias\nbert.encoder.layer.10.output.dense.bias→encoder.layer.10.output.LayerNorm.gamma\nbert.encoder.layer.10.output.LayerNorm.gamma→encoder.layer.10.output.LayerNorm.beta\nbert.encoder.layer.10.output.LayerNorm.beta→encoder.layer.11.attention.selfattn.query.weight\nbert.encoder.layer.11.attention.self.query.weight→encoder.layer.11.attention.selfattn.query.bias\nbert.encoder.layer.11.attention.self.query.bias→encoder.layer.11.attention.selfattn.key.weight\nbert.encoder.layer.11.attention.self.key.weight→encoder.layer.11.attention.selfattn.key.bias\nbert.encoder.layer.11.attention.self.key.bias→encoder.layer.11.attention.selfattn.value.weight\nbert.encoder.layer.11.attention.self.value.weight→encoder.layer.11.attention.selfattn.value.bias\nbert.encoder.layer.11.attention.self.value.bias→encoder.layer.11.attention.output.dense.weight\nbert.encoder.layer.11.attention.output.dense.weight→encoder.layer.11.attention.output.dense.bias\nbert.encoder.layer.11.attention.output.dense.bias→encoder.layer.11.attention.output.LayerNorm.gamma\nbert.encoder.layer.11.attention.output.LayerNorm.gamma→encoder.layer.11.attention.output.LayerNorm.beta\nbert.encoder.layer.11.attention.output.LayerNorm.beta→encoder.layer.11.intermediate.dense.weight\nbert.encoder.layer.11.intermediate.dense.weight→encoder.layer.11.intermediate.dense.bias\nbert.encoder.layer.11.intermediate.dense.bias→encoder.layer.11.output.dense.weight\nbert.encoder.layer.11.output.dense.weight→encoder.layer.11.output.dense.bias\nbert.encoder.layer.11.output.dense.bias→encoder.layer.11.output.LayerNorm.gamma\nbert.encoder.layer.11.output.LayerNorm.gamma→encoder.layer.11.output.LayerNorm.beta\nbert.encoder.layer.11.output.LayerNorm.beta→pooler.dense.weight\nbert.pooler.dense.weight→pooler.dense.bias\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertModel:\n\tsize mismatch for embeddings.LayerNorm.gamma: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for pooler.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for pooler.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-54a4c4cf078e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m net_bert = set_learned_params(\n\u001b[0;32m----> 8\u001b[0;31m     net_bert, weights_path = '/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/weights/pytorch_model.bin')\n\u001b[0m",
      "\u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/utils/bert.py\u001b[0m in \u001b[0;36mset_learned_params\u001b[0;34m(net, weights_path)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# 新たなstate_dictを構築したBERTモデルに与える\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertModel:\n\tsize mismatch for embeddings.LayerNorm.gamma: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.query.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.query.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.key.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.key.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.value.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.selfattn.value.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for pooler.dense.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for pooler.dense.bias: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([768])."
     ]
    }
   ],
   "source": [
    "from utils.bert import get_config, set_learned_params\n",
    "\n",
    "config = get_config(file_path='/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/weights/bert_config.json')\n",
    "\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "net_bert = set_learned_params(\n",
    "    net_bert, weights_path = '/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/weights/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNLP(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForNLP, self).__init__()\n",
    "\n",
    "        self.bert = net_bert\n",
    "\n",
    "        self.cls = nn.Linear(in_features=768, out_features=4)\n",
    "\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None,\n",
    "                attention_mask=None, output_all_encoded_layers=False,\n",
    "                attention_show_flg=False):\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                        input_ids, token_type_ids, attention_mask,\n",
    "                        output_all_encoded_layers, attention_show_flg)\n",
    "                \n",
    "                elif attention_show_flg == False:\n",
    "                     encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                        input_ids, token_type_ids, attention_mask,\n",
    "                        output_all_encoded_layers, attention_show_flg)\n",
    "                \n",
    "                vec_0 = encoded_layers[:, 0, :]\n",
    "                vec_0 = vec_0.view(-1, 768)\n",
    "                out = self.cls(vec_0)\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    return out, attention_probs\n",
    "                \n",
    "                if attention_show_flg == False:\n",
    "                    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertForNLP(\n  (bert): BertModel(\n    (embedding): BertEmbedding(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (selfattn): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (cls): Linear(in_features=768, out_features=4, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "net = BertForNLP(net_bert)\n",
    "\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr':5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr':5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df_news = pd.read_csv('/Users/ryomisawa/Downloads/NewsAggregatorDataset/newsCorpora.csv', sep = '\\t', names=col_names)\n",
    "\n",
    "df_news = df_news[(df_news['PUBLISHER'] == 'Reuters') | (df_news['PUBLISHER'] ==  'Huffington Post')|(df_news['PUBLISHER'] == 'Businessweek') | (df_news['PUBLISHER'] == 'Contactmusic.com') |(df_news['PUBLISHER'] == 'Daily Mail')].sample(frac=1, random_state=0).reset_index()\n",
    "\n",
    "df_news['CATEGORY'].replace('b', 0, inplace=True)\n",
    "df_news['CATEGORY'].replace('t', 1, inplace=True)\n",
    "df_news['CATEGORY'].replace('e', 2, inplace=True)\n",
    "df_news['CATEGORY'].replace('m', 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True,\n",
    "                            tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True,\n",
    "                            fix_length=max_length, init_token=\"[CLS]\",\n",
    "                            eos_token=\"[SEP]\", pad_token='[PAD]',\n",
    "                            unk_token='[UNK]')\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='/Users/ryomisawa/nlp_tutorial/', train='news_train.csv',\n",
    "    test='news_test.csv',format='csv', \n",
    "    fields=[('Text',TEXT), ('Label', LABEL)])\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(\n",
    "    split_ratio=0.89, random_state=random.seed(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bert, ids_to_tokens_bert = load_vocab(\n",
    "    vocab_file='/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt')\n",
    "\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dl, 'val': val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'FOREX-Dollar stung by Fed minutes, Aussie rises on solid jobs data'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-2bd7a6429bd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, str)\n\u001b[0;32m--> 352\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 arr = [numericalization_func(x) if isinstance(x, str)\n\u001b[0;32m--> 352\u001b[0;31m                        else x for x in arr]\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'FOREX-Dollar stung by Fed minutes, Aussie rises on solid jobs data'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600737874335",
   "display_name": "Python 3.7.7 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}