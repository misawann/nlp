{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting torchtext\n  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n\u001b[?25hRequirement already satisfied: numpy in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.18.1)\nRequirement already satisfied: torch in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.4.0)\nRequirement already satisfied: tqdm in /Users/ryomisawa/.local/lib/python3.6/site-packages (from torchtext) (4.46.1)\nRequirement already satisfied: six in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (1.14.0)\nRequirement already satisfied: requests in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from torchtext) (2.23.0)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.91-cp36-cp36m-macosx_10_6_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 7.1 MB/s \n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (2020.4.5.1)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /Users/ryomisawa/opt/anaconda3/envs/practice/lib/python3.6/site-packages (from requests->torchtext) (2.9)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.91 torchtext-0.6.0\n"
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F \n",
    "import torchtext \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "def torch_log(x):\n",
    "    return torch.log(torch.clamp(x, min=1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df_news = pd.read_csv('/Users/ryomisawa/Downloads/NewsAggregatorDataset/newsCorpora.csv', sep = '\\t', names=col_names)\n",
    "\n",
    "df_news = df_news[(df_news['PUBLISHER'] == 'Reuters') | (df_news['PUBLISHER'] ==  'Huffington Post')|(df_news['PUBLISHER'] == 'Businessweek') | (df_news['PUBLISHER'] == 'Contactmusic.com') |(df_news['PUBLISHER'] == 'Daily Mail')].sample(frac=1, random_state=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.drop('ID', axis=1, inplace=True)\n",
    "df_news.drop('URL', axis=1, inplace=True)\n",
    "df_news.drop('PUBLISHER', axis=1, inplace=True)\n",
    "df_news.drop('STORY', axis=1, inplace=True)\n",
    "df_news.drop('HOSTNAME', axis=1, inplace=True)\n",
    "df_news.drop('TIMESTAMP', axis=1, inplace=True)\n",
    "df_news.drop('index', axis=1, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_news_train, df_news_test = train_test_split(df_news, train_size=0.9)\n",
    "\n",
    "import os \n",
    "os.chdir('/Users/ryomisawa/nlp_tutorial')\n",
    "df_news_train.to_csv('news_train.csv')\n",
    "df_news_test.to_csv('news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_news['TITLE']\n",
    "df_y = df_news['CATEGORY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid, y_train, y_valid = train_test_split(df_x, df_y, test_size=0.2)\n",
    "df_valid, df_test, y_valid, y_test = train_test_split(df_valid, y_valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "  text = re.sub('<br />', '', text)\n",
    "\n",
    "  for p in string.punctuation:\n",
    "      text = text.replace(p, ' ')\n",
    "  return text\n",
    "\n",
    "def tokenizer_punctuation(text):\n",
    "  return text.strip().split()\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "  text = preprocessing_text(text)\n",
    "  ret = tokenizer_punctuation(text)\n",
    "  return ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "texts = tokenizer_with_preprocessing(df_train.to_string(index=False))\n",
    "texts = collections.Counter(texts).most_common()\n",
    "\n",
    "text_id = {}\n",
    "for i in range(len(texts)):\n",
    "    if texts[i][1] > 1:\n",
    "        text_id[texts[i][0]] = i+1\n",
    "    else:\n",
    "        text_id[texts[i][0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データが多すぎて時間がかかるためデータ削減\n",
    "df_train, df_nouse, y_train, y_nouse = train_test_split(df_x, df_y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2id(text):\n",
    "    ids = []\n",
    "    for word in text:\n",
    "        if word in text_id:\n",
    "            ids.append(text_id[word])\n",
    "        else:\n",
    "            ids.append(0)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_id = []\n",
    "for sentence in df_train:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_train_id.append(word2id(sentence))\n",
    "x_valid_id = []\n",
    "for sentence in df_valid:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_valid_id.append(word2id(sentence))\n",
    "#x_valid_oh = torch.tensor(x_valid_oh)\n",
    "x_test_id = []\n",
    "for sentence in df_test:\n",
    "    sentence = tokenizer_with_preprocessing(sentence)\n",
    "    x_test_id.append(word2id(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s) for s in x_train_id])\n",
    "for i in range(len(x_train_id)):\n",
    "    x_train_id[i] = x_train_id[i] + [0 for i in range(max_length - len(x_train_id[i]))]\n",
    "x_train_id = torch.tensor(x_train_id)\n",
    "\n",
    "max_length = max([len(s) for s in x_valid_id])\n",
    "for i in range(len(x_valid_id)):\n",
    "    x_valid_id[i] = x_valid_id[i] + [0 for i in range(max_length - len(x_valid_id[i]))]\n",
    "x_valid_id = torch.tensor(x_valid_id)\n",
    "\n",
    "max_length = max([len(s) for s in x_test_id])\n",
    "for i in range(len(x_test_id)):\n",
    "    x_test_id[i] = x_test_id[i] + [0 for i in range(max_length - len(x_test_id[i]))]\n",
    "x_test_id = torch.tensor(x_test_id)\n",
    "\n",
    "x_train_id = x_train_id.long()\n",
    "x_valid_id = torch.tensor(x_valid_id).long()\n",
    "x_test_id = torch.tensor(x_test_id).long()\n",
    "\n",
    "len_seq_train = torch.tensor([len(s) for s in x_train_id]) \n",
    "len_seq_valid = torch.tensor([len(s) for s in x_valid_id])\n",
    "len_seq_test = torch.tensor([len(s) for s in x_test_id])\n",
    "\n",
    "t_train = []\n",
    "t_valid = []\n",
    "t_test = []\n",
    "category = ['b', 'e', 't', 'm']\n",
    "number = [0, 1, 2, 3]\n",
    "n_labels = len(number)\n",
    "for text in y_train.replace(category, number):\n",
    "    t_train.append(text)\n",
    "t_train = torch.from_numpy(np.eye(n_labels)[t_train]).long()\n",
    "for text in y_valid.replace(category, number):\n",
    "    t_valid.append(text)\n",
    "t_valid = torch.from_numpy(np.eye(n_labels)[t_valid]).long()\n",
    "for text in y_test.replace(category, number):\n",
    "    t_test.append(text)\n",
    "t_test = torch.from_numpy(np.eye(n_labels)[t_test]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_matrix = nn.Parameter(torch.rand((vocab_size, emb_dim), dtype=torch.float))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.embedding(x, self.embedding_matrix)\n",
    "        \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  \n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  \n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  \n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        for i in range(len_seq_max):\n",
    "            state = self.function(state, x[i])\n",
    "            output = torch.cat([output, state.unsqueeze(0)])  \n",
    "        return output\n",
    "class SequenceTaggingNet(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.emb = Embedding(emb_dim, word_num)\n",
    "        self.rnn = RNN(emb_dim, hid_dim)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = self.emb(x)\n",
    "        h = self.rnn(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.579, accuracy_train:0.397,loss_test:0.531, accuracy_test:0.409 \nloss_train:0.535, accuracy_train:0.397,loss_test:0.506, accuracy_test:0.409 \nloss_train:0.507, accuracy_train:0.397,loss_test:0.496, accuracy_test:0.418 \nloss_train:0.495, accuracy_train:0.427,loss_test:0.499, accuracy_test:0.411 \nloss_train:0.496, accuracy_train:0.427,loss_test:0.502, accuracy_test:0.411 \nloss_train:0.498, accuracy_train:0.427,loss_test:0.500, accuracy_test:0.411 \nloss_train:0.496, accuracy_train:0.427,loss_test:0.497, accuracy_test:0.411 \nloss_train:0.494, accuracy_train:0.427,loss_test:0.494, accuracy_test:0.413 \nloss_train:0.492, accuracy_train:0.427,loss_test:0.494, accuracy_test:0.408 \nloss_train:0.492, accuracy_train:0.397,loss_test:0.493, accuracy_test:0.409 \n"
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()\n",
    "\n",
    "    net.zero_grad()\n",
    "\n",
    "    t = t_train.to(device)\n",
    "    x = x_train_id.to(device)\n",
    "    h = net(x, torch.max(len_seq_train), len_seq_train)\n",
    "    y = softmax(h).squeeze()\n",
    "\n",
    "    loss_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "    loss_train.backward()  # 誤差の逆伝播\n",
    "        \n",
    "    optimizer.step()  # パラメータの更新\n",
    "\n",
    "    acc_train = accuracy_score(t.argmax(axis=1), y.argmax(axis=1))\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    t = t_test.to(device)\n",
    "    x = x_test_id.to(device)\n",
    "    h = net(x, torch.max(len_seq_test), len_seq_test)\n",
    "    y = softmax(h).squeeze()\n",
    "\n",
    "    loss_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "    \n",
    "    acc_test = accuracy_score(t.argmax(axis=1), y.argmax(axis=1))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(loss_train, acc_train, loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.511, accuracy_train:0.407,loss_test:0.495, accuracy_test:0.409 \nloss_train:0.508, accuracy_train:0.415,loss_test:0.496, accuracy_test:0.410 \nloss_train:0.507, accuracy_train:0.407,loss_test:0.492, accuracy_test:0.411 \nloss_train:0.508, accuracy_train:0.422,loss_test:0.510, accuracy_test:0.411 \nloss_train:0.508, accuracy_train:0.413,loss_test:0.508, accuracy_test:0.409 \nloss_train:0.511, accuracy_train:0.419,loss_test:0.502, accuracy_test:0.409 \nloss_train:0.508, accuracy_train:0.418,loss_test:0.495, accuracy_test:0.411 \nloss_train:0.510, accuracy_train:0.404,loss_test:0.499, accuracy_test:0.410 \nloss_train:0.505, accuracy_train:0.409,loss_test:0.515, accuracy_test:0.409 \nloss_train:0.509, accuracy_train:0.413,loss_test:0.492, accuracy_test:0.411 \n"
    }
   ],
   "source": [
    "import time\n",
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.tensor([len(s) for s in x])\n",
    "        h = net(x, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    \n",
    "    net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.tensor([len(s) for s in x])\n",
    "        h = net(x, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/ryomisawa/Downloads/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "\n",
    "train_embedding = []\n",
    "max_length_train = max([len(s.split()) for s in df_train])\n",
    "\n",
    "for s in df_train:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    train_embedding.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding = torch.tensor(train_embedding).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "valid_embedding = []\n",
    "max_length_valid = max([len(s.split()) for s in df_valid])\n",
    "\n",
    "for s in df_valid:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length_valid - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    valid_embedding.append(x)\n",
    "\n",
    "valid_embedding = torch.tensor(valid_embedding).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "test_embedding = []\n",
    "max_length_test = max([len(s.split()) for s in df_test])\n",
    "for s in df_test:\n",
    "    s = s.split()\n",
    "    x = []\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            x.append(model[w])\n",
    "        else:\n",
    "            x.append(np.zeros(d))\n",
    "    for i in range(max_length_test - len(x)):\n",
    "        x.append(np.zeros(d))\n",
    "    test_embedding.append(x)\n",
    "\n",
    "test_embedding = torch.tensor(test_embedding).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet2(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = RNN(emb_dim, hid_dim)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, embedding, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        h = self.rnn(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.508, accuracy_train:0.417,loss_test:0.498, accuracy_test:0.421 \nloss_train:0.507, accuracy_train:0.418,loss_test:0.502, accuracy_test:0.411 \nloss_train:0.505, accuracy_train:0.405,loss_test:0.527, accuracy_test:0.409 \nloss_train:0.507, accuracy_train:0.410,loss_test:0.498, accuracy_test:0.411 \nloss_train:0.513, accuracy_train:0.399,loss_test:0.504, accuracy_test:0.409 \nloss_train:0.512, accuracy_train:0.410,loss_test:0.516, accuracy_test:0.411 \nloss_train:0.511, accuracy_train:0.416,loss_test:0.500, accuracy_test:0.411 \nloss_train:0.513, accuracy_train:0.408,loss_test:0.523, accuracy_test:0.409 \nloss_train:0.507, accuracy_train:0.409,loss_test:0.508, accuracy_test:0.410 \nloss_train:0.515, accuracy_train:0.402,loss_test:0.511, accuracy_test:0.410 \n"
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        glorot = 6/(in_dim + hid_dim*2)\n",
    "        self.W = nn.Parameter(torch.tensor(rng.uniform(\n",
    "                        low=-np.sqrt(glorot),\n",
    "                        high=np.sqrt(glorot),\n",
    "                        size=(in_dim + hid_dim, hid_dim)\n",
    "                    ).astype('float32')))\n",
    "        self.b = nn.Parameter(torch.tensor(np.zeros([hid_dim]).astype('float32')))\n",
    "\n",
    "    def function(self, h, x):\n",
    "        return torch.tanh(torch.matmul(torch.cat([h, x], dim=1), self.W) + self.b)\n",
    "\n",
    "    def forward(self, x, len_seq_max=0, init_state=None):\n",
    "        x = x.transpose(0, 1)  # 系列のバッチ処理のため、次元の順番を「系列、バッチ」の順に入れ替える\n",
    "        state = init_state\n",
    "        \n",
    "        if init_state is None:  # 初期値を設定しない場合は0で初期化する\n",
    "            state = torch.zeros((x[0].size()[0], self.hid_dim)).to(x.device)\n",
    "\n",
    "        size = list(state.unsqueeze(0).size())\n",
    "        size[0] = 0\n",
    "        output = torch.empty(size, dtype=torch.float).to(x.device)  # 一旦空テンソルを定義して順次出力を追加する\n",
    "\n",
    "        if len_seq_max == 0:\n",
    "            len_seq_max = x.size(0)\n",
    "        \n",
    "        for i in range(len_seq_max):\n",
    "            state = torch.tanh(torch.matmul(torch.cat([state, x[i]], dim=1), W) + b)\n",
    "            output = torch.cat([output, state.unsqueeze(0)])\n",
    "        return output\n",
    "\n",
    "class SequenceTaggingNet2(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn_1 = RNN(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.rnn_2 = RNN(hid_dim, hid_dim, bidirectional=True)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self, x, embedding, len_seq_max=0, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        h = self.rnn_1(h, len_seq_max, init_state)\n",
    "        if len_seq is not None:\n",
    "            h = h[len_seq-1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "        return y\n",
    "\n",
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet2(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTaggingNet3(nn.Module):\n",
    "    def __init__(self, word_num, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(emb_dim, hid_dim, 4, batch_first=True)\n",
    "        self.linear = nn.Linear(hid_dim, 4)\n",
    "    \n",
    "    def forward(self,x, embedding,  len_seq_max, len_seq=None, init_state=None):\n",
    "        h = embedding\n",
    "        if len_seq_max > 0:\n",
    "            h, _ = self.rnn(h[:, 0:len_seq_max, :], init_state)\n",
    "        else:\n",
    "            h, _ = self.rnn(h, init_state)\n",
    "        h = h.transpose(0, 1)\n",
    "        if len_seq is None:\n",
    "            h = h[len_seq - 1, list(range(len(x))), :]\n",
    "        else:\n",
    "            h = h[-1]\n",
    "        y = self.linear(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:0.461, accuracy_train:0.602,loss_test:0.433, accuracy_test:0.651 \nloss_train:0.447, accuracy_train:0.629,loss_test:0.428, accuracy_test:0.651 \nloss_train:0.447, accuracy_train:0.632,loss_test:0.427, accuracy_test:0.651 \nloss_train:0.448, accuracy_train:0.639,loss_test:0.459, accuracy_test:0.652 \nloss_train:0.450, accuracy_train:0.633,loss_test:0.430, accuracy_test:0.652 \nloss_train:0.448, accuracy_train:0.636,loss_test:0.462, accuracy_test:0.651 \nloss_train:0.451, accuracy_train:0.634,loss_test:0.441, accuracy_test:0.650 \nloss_train:0.452, accuracy_train:0.622,loss_test:0.481, accuracy_test:0.651 \nloss_train:0.448, accuracy_train:0.625,loss_test:0.472, accuracy_test:0.651 \nloss_train:0.450, accuracy_train:0.627,loss_test:0.434, accuracy_test:0.651 \n"
    }
   ],
   "source": [
    "word_num = len(texts)\n",
    "emb_dim = 300\n",
    "hid_dim = 50\n",
    "n_epochs = 10\n",
    "\n",
    "num_data_train = len(x_train_id)\n",
    "num_data_test = len(x_test_id)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = SequenceTaggingNet3(word_num, emb_dim, hid_dim)\n",
    "net.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    t1 = time.time()\n",
    "    sff_idx_train = np.random.permutation(num_data_train)\n",
    "    sff_idx_test = np.random.permutation(num_data_test)\n",
    "\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    net.train()\n",
    "    for idx in range(0, num_data_train, batch_size):\n",
    "        net.zero_grad()\n",
    "\n",
    "        x = x_train_id[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        embedding = train_embedding[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        t = t_train[sff_idx_train[idx:idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "        len_seq_train = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_train), len_seq_train)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_train = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "        l_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(l_train.item())\n",
    "        acc_train.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "        \n",
    "        net.eval()\n",
    "    for idx in range(0, num_data_test, batch_size):\n",
    "        x = x_test_id[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        embedding = test_embedding[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        t = t_test[sff_idx_test[idx:idx + batch_size if idx + batch_size < num_data_test else num_data_test]].to(device)\n",
    "        len_seq_test = torch.LongTensor([len(s) for s in x])\n",
    "\n",
    "        h = net(x, embedding, torch.max(len_seq_test), len_seq_test)\n",
    "        y = softmax(h).squeeze()\n",
    "\n",
    "        l_test = -torch.mean(t*torch_log(y) + (1-t)*torch_log(1-y))\n",
    "\n",
    "        loss_test.append(l_test.item())\n",
    "        acc_test.append(accuracy_score(t.argmax(axis=1), y.argmax(axis=1)))\n",
    "    print('loss_train:{:.3f}, accuracy_train:{:.3f},loss_test:{:.3f}, accuracy_test:{:.3f} '.format(np.mean(loss_train), np.mean(acc_train), np.mean(loss_test), np.mean(acc_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = 50\n",
    "L = 4\n",
    "b_1 = nn.Parameter(torch.tensor(np.zeros((d_out), dtype='float32')))\n",
    "b_2 = nn.Parameter(torch.tensor(np.zeros((L), dtype='float32')))\n",
    "W_1 = torch.randn(d_out, 3*d, requires_grad=True)\n",
    "W_2 = torch.randn(L, d_out, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loss_train:6.219, acc_train:0.380, loss_valid:6.277, acc_valid:0.377\nloss_train:6.202, acc_train:0.378, loss_valid:6.251, acc_valid:0.376\nloss_train:6.182, acc_train:0.378, loss_valid:6.232, acc_valid:0.374\nloss_train:6.166, acc_train:0.376, loss_valid:6.216, acc_valid:0.375\nloss_train:6.150, acc_train:0.377, loss_valid:6.202, acc_valid:0.372\n"
    }
   ],
   "source": [
    "epochs = 5\n",
    "optimizer = optim.Adam([b_1, b_2, W_1, W_2], lr=0.001)\n",
    "f = nn.ReLU()\n",
    "y_pred_train = []\n",
    "y_pred_test = []\n",
    "\n",
    "m = nn.Softmax(dim=0)\n",
    "for epoch in range(epochs):\n",
    "    l_train = 0\n",
    "    acc_train = 0\n",
    "    for i in range(len(train_embedding)):\n",
    "        e = train_embedding[i]\n",
    "        c = []\n",
    "        p = []\n",
    "        for t in range(1, len(e)-1):\n",
    "            concat = torch.cat((e[t-1], e[t], e[t+1]), 0)\n",
    "            p.append(f(torch.matmul(W_1, concat) + b_1))\n",
    "        for l in range(d_out):\n",
    "            c.append(max([s[l].item() for s in p]))\n",
    "        c = torch.tensor(c).float()\n",
    "        y = m(torch.matmul(W_2, c) + b_2)\n",
    "        l_train += -(t_train[i]*torch_log(y) + (1-t_train[i])*torch_log(1-y))\n",
    "        if t_train[i].argmax(axis=0).item() == y.argmax(axis=0).item():\n",
    "            acc_train += 1\n",
    "    l_train = torch.mean(l_train/len(train_embedding))\n",
    "    optimizer.zero_grad()\n",
    "    l_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc_train = acc_train/len(train_embedding)\n",
    "\n",
    "    l_valid = 0\n",
    "    acc_valid = 0\n",
    "    for i in range(len(valid_embedding)):\n",
    "        e = valid_embedding[i]\n",
    "        c = []\n",
    "        p = []\n",
    "        for t in range(1, len(e)-1):\n",
    "            concat = torch.cat((e[t-1], e[t], e[t+1]), 0)\n",
    "            p.append(f(torch.matmul(W_1, concat) + b_1))\n",
    "        for l in range(d_out):\n",
    "            c.append(max([s[l].item() for s in p]))\n",
    "        c = torch.tensor(c).float()\n",
    "        y = m(torch.matmul(W_2, c) + b_2)\n",
    "        l_valid += -(t_valid[i]*torch_log(y) + (1-t_valid[i])*torch_log(1-y))\n",
    "        if t_valid[i].argmax(axis=0).item() == y.argmax(axis=0).item():\n",
    "            acc_valid +=1\n",
    "    l_valid = torch.mean(l_valid/len(valid_embedding))\n",
    "    acc_valid = acc_valid/len(valid_embedding)\n",
    "\n",
    "    print('loss_train:{:.3f}, acc_train:{:.3f}, loss_valid:{:.3f}, acc_valid:{:.3f}'.format(l_train.item(), acc_train ,l_valid.item(), acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "config_file = \"/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/weights/bert_config.json\"\n",
    "\n",
    "json_file = open(config_file, 'r')\n",
    "config = json.load(json_file)\n",
    "\n",
    "from attrdict import AttrDict\n",
    "\n",
    "config = AttrDict(config)\n",
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        \n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings,config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "\n",
    "        self.attention = BertAttention(config)\n",
    "\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "\n",
    "        self.output = BertOutput(config)\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        \n",
    "        if attention_show_flg == True:\n",
    "            attention_output, attention_probs = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            return layer_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            attention_output = self.attention(\n",
    "                hidden_states, attention_mask, attention_show_flg)\n",
    "            intermediate_output = self.intermediate(attention_output)\n",
    "            layer_output = self.output(intermediate_output, attention_output)\n",
    "            \n",
    "            return layer_output\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.selfattn = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "    \n",
    "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
    "        \n",
    "        if attention_show_flg ==True:\n",
    "            self_output, attention_probs = self.selfattn(input_tensor, \n",
    "                                                         attention_mask,\n",
    "                                                         attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            self_output = self.selfattn(input_tensor, attention_mask,\n",
    "                                        attention_show_flg)\n",
    "            attention_output = self.output(self_output, input_tensor)\n",
    "            return attention_output \n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "\n",
    "        self.attention_head_size = int(\n",
    "            config.hidden_size / config.num_attention_heads)\n",
    "        \n",
    "        self.all_head_size = self.num_attention_heads *  \\\n",
    "          self.attention_head_size\n",
    "        \n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "    \n",
    "    def transpose_for_scores(self, x):\n",
    "\n",
    "        new_x_shape = x.size()[\n",
    "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(\n",
    "            query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / \\\n",
    "            math.sqrt(self.attention_head_size)\n",
    "\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[\n",
    "            :-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        if attention_show_flg == True:\n",
    "            return context_layer, attention_probs\n",
    "        \n",
    "        elif attention_show_flg == False:\n",
    "            return context_layer\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "\n",
    "        self.intermediate_act_fn = gelu\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "class BertOutput(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        self.layer = nn.ModuleList([BertLayer(config)\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask,\n",
    "                output_all_encoded_layers=True, attention_show_flg=False):\n",
    "               \n",
    "                all_encoder_layers = []\n",
    "\n",
    "                for layer_module in self.layer:\n",
    "\n",
    "                    if attention_show_flg == True:\n",
    "                        hidden_states, attention_probs = layer_module(\n",
    "                            hidden_states, attention_mask, attention_show_flg)\n",
    "\n",
    "                    elif attention_show_flg == False:\n",
    "                        hidden_states = layer_module(\n",
    "                            hidden_states, attention_mask, attention_show_flg)\n",
    "                    \n",
    "                    if output_all_encoded_layers:\n",
    "                        all_encoder_layers.append(hidden_states)\n",
    "                \n",
    "                if not output_all_encoded_layers:\n",
    "                    all_encoder_layers.append(hidden_states)\n",
    "                \n",
    "                if attention_show_flg == True:\n",
    "                    return all_encoder_layers, attention_probs\n",
    "                elif attention_show_flg == False:\n",
    "                    return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "入力の単語ID列のテンソルサイズ： torch.Size([2, 5])\n入力のマスクのテンソルサイズ： torch.Size([2, 5])\n入力の文章IDのテンソルサイズ： torch.Size([2, 5])\n拡張したマスクのテンソルサイズ： torch.Size([2, 1, 1, 5])\nBertEmbeddingsの出力テンソルサイズ： torch.Size([2, 5, 768])\nBertEncoderの最終層の出力テンソルサイズ： torch.Size([2, 5, 768])\nBertPoolerの出力テンソルサイズ： torch.Size([2, 768])\n"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
    "\n",
    "# マスク\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "print(\"入力のマスクのテンソルサイズ：\", attention_mask.shape)\n",
    "\n",
    "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
    "\n",
    "\n",
    "# BERTの各モジュールを用意\n",
    "embeddings = BertEmbeddings(config)\n",
    "encoder = BertEncoder(config)\n",
    "pooler = BertPooler(config)\n",
    "\n",
    "# マスクの変形　[batch_size, 1, 1, seq_length]にする\n",
    "# Attentionをかけない部分はマイナス無限にしたいので、代わりに-10000をかけ算しています\n",
    "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
    "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "print(\"拡張したマスクのテンソルサイズ：\", extended_attention_mask.shape)\n",
    "\n",
    "# 順伝搬する\n",
    "out1 = embeddings(input_ids)\n",
    "print(\"BertEmbeddingsの出力テンソルサイズ：\", out1.shape)\n",
    "\n",
    "out2 = encoder(out1, extended_attention_mask)\n",
    "# out2は、[minibatch, seq_length, embedding_dim]が12個のリスト\n",
    "print(\"BertEncoderの最終層の出力テンソルサイズ：\", out2[0].shape)\n",
    "\n",
    "out3 = pooler(out2[-1])  # out2は12層の特徴量のリストになっているので一番最後を使用\n",
    "print(\"BertPoolerの出力テンソルサイズ：\", out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None,output_all_encoded_layers=True, attention_show_flg=False):\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones_like(input_ids)\n",
    "                \n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "                extended_attention_mask = extended_attention_mask.to(\n",
    "                    dtype=torch.float32)\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * - 10000.0\n",
    "\n",
    "                embedding_output = self.embeddings(input_ids)\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    encoded_layers, attention_probs = self.encoder(embedding_output,\n",
    "                                                                   extended_attention_mask,\n",
    "                                                                   output_all_encoded_layers,\n",
    "                                                                   attention_show_flg)\n",
    "                elif attention_show_flg == False:\n",
    "                    encoded_layers = self.encoder(embedding_output,\n",
    "                                                  extended_attention_mask,\n",
    "                                                  output_all_encoded_layers,\n",
    "                                                  attention_show_flg)\n",
    "                \n",
    "                pooled_output = self.pooler(encoded_layers[-1])\n",
    "\n",
    "                if not output_all_encoded_layers:\n",
    "                    encoded_layers = encoded_layers[-1]\n",
    "                \n",
    "                if attention_show_flg == True:\n",
    "                    return encoded_layers, pooled_output, attention_probs\n",
    "                elif attention_show_flg == False:\n",
    "                    return encoded_layers, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
    "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
    "\n",
    "# BERTモデルを作る\n",
    "net = BertModel(config)\n",
    "\n",
    "# 順伝搬させる\n",
    "encoded_layers, pooled_output, attention_probs= net(input_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "print(\"encoded_layersのテンソルサイズ：\", encoded_layers.shape)\n",
    "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
    "print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    ids_to_tokens = collections.OrderedDict()\n",
    "    index = 0\n",
    "\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "\n",
    "            vocab[token] = index\n",
    "            ids_to_tokens[index] = token\n",
    "            index += 1\n",
    "    return vocab, ids_to_tokens\n",
    "\n",
    "vocab_file = '/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt'\n",
    "vocab, ids_to_tokens = load_vocab(vocab_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert')\n",
    "\n",
    "from utils.tokenizer import BasicTokenizer, WordpieceTokenizer\n",
    "\n",
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case=True):\n",
    "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
    "\n",
    "        never_split = ('[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]')\n",
    "\n",
    "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
    "                                              never_split=never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text):\n",
    "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "                split_tokens.append(sub_token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab[token])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            tokens.append(self.ids_to_tokens[i])\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "\n",
    "    for p in string.punctuation:\n",
    "        if (p == '.') or (p == ','):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, ' ')\n",
    "    \n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    return text\n",
    "\n",
    "tokenizer_bert = BertTokenizer(\n",
    "    vocab_file='/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt', do_lower_case=True)\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bert import get_config, set_learned_params, BertModel\n",
    "\n",
    "config = get_config(file_path='weights/bert_config.json')\n",
    "\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "net_bert = set_learned_params(\n",
    "    net_bert, weights_path = 'weights/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNLP(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForNLP, self).__init__()\n",
    "\n",
    "        self.bert = net_bert\n",
    "\n",
    "        self.cls = nn.Linear(in_features=768, out_features=4)\n",
    "\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False,attention_show_flg=False):\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    encoded_layers, pooled_output, attention_probs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "                \n",
    "                elif attention_show_flg == False:\n",
    "                    encoded_layers, pooled_output, attention_probs = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "                \n",
    "                vec_0 = encoded_layers[:, 0, :]\n",
    "                vec_0 = vec_0.view(-1, 768)\n",
    "                out = self.cls(vec_0)\n",
    "\n",
    "                if attention_show_flg == True:\n",
    "                    return out, attention_probs\n",
    "                \n",
    "                if attention_show_flg == False:\n",
    "                    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNLP(nn.Module):\n",
    "    '''BERTモデルにIMDbのポジ・ネガを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForNLP, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル\n",
    "\n",
    "        # headにポジネガ予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元、出力はポジ・ネガの2つ\n",
    "        self.cls = nn.Linear(in_features=768, out_features=4)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            '''attention_showのときは、attention_probsもリターンする'''\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "\n",
    "        # 入力文章の1単語目[CLS]の特徴量を使用して、ポジ・ネガを分類します\n",
    "        vec_0 = encoded_layers[:, 0, :]\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
    "        out = self.cls(vec_0)\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BertForNLP(net_bert)\n",
    "\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr':5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr':5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df_news = pd.read_csv('/Users/ryomisawa/Downloads/NewsAggregatorDataset/newsCorpora.csv', sep = '\\t', names=col_names)\n",
    "\n",
    "df_news = df_news[(df_news['PUBLISHER'] == 'Reuters') | (df_news['PUBLISHER'] ==  'Huffington Post')|(df_news['PUBLISHER'] == 'Businessweek') | (df_news['PUBLISHER'] == 'Contactmusic.com') |(df_news['PUBLISHER'] == 'Daily Mail')].sample(frac=1, random_state=0).reset_index()\n",
    "\n",
    "df_news['CATEGORY'].replace('b', 0, inplace=True)\n",
    "df_news['CATEGORY'].replace('t', 1, inplace=True)\n",
    "df_news['CATEGORY'].replace('e', 2, inplace=True)\n",
    "df_news['CATEGORY'].replace('m', 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_train, df_news_test = train_test_split(df_news, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_train.to_csv('/Users/ryomisawa/nlp_tutorial/news_train.csv', columns=['TITLE','CATEGORY'],header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_test.to_csv('/Users/ryomisawa/nlp_tutorial/news_test.csv', columns=['TITLE','CATEGORY'],header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True,\n",
    "                            tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True,\n",
    "                            fix_length=max_length, init_token=\"[CLS]\",\n",
    "                            eos_token=\"[SEP]\", pad_token='[PAD]',\n",
    "                            unk_token='[UNK]')\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='/Users/ryomisawa/nlp_tutorial/', train='news_train.csv',\n",
    "    test='news_test.csv',format='csv', \n",
    "    fields=[('Text',TEXT), ('Label', LABEL)])\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(\n",
    "    split_ratio=0.89, random_state=random.seed(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bert, ids_to_tokens_bert = load_vocab(\n",
    "    vocab_file='/Users/ryomisawa/Library/Mobile Documents/com~apple~CloudDocs/pytorch/pytorch_advanced-master 2/8_nlp_sentiment_bert/vocab/bert-base-uncased-vocab.txt')\n",
    "\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "val_dl = torchtext.data.Iterator(\n",
    "    val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "test_dl = torchtext.data.Iterator(\n",
    "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dataloaders_dict = {'train': train_dl, 'val': val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloader_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('使用デバイス', device)\n",
    "    print('-----start-----')\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    batch_size = dataloader_dict['train'].batch_size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BertForIMDbに入力\n",
    "                    outputs = net(inputs,token_type_ids=None,attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            t_epoch_start = time.time()\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('nlp': conda)",
   "display_name": "Python 3.7.7 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}