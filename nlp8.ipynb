{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "df_news = pd.read_csv('/Users/ryomisawa/Downloads/NewsAggregatorDataset/newsCorpora.csv', sep = '\\t', names=col_names)\n",
    "\n",
    "df_news = df_news[(df_news['PUBLISHER'] == 'Reuters') | (df_news['PUBLISHER'] ==  'Huffington Post')|(df_news['PUBLISHER'] == 'Businessweek') | (df_news['PUBLISHER'] == 'Contactmusic.com') |(df_news['PUBLISHER'] == 'Daily Mail')].sample(frac=1, random_state=0).reset_index()\n",
    "\n",
    "df_news['CATEGORY'].replace('b', 0, inplace=True)\n",
    "df_news['CATEGORY'].replace('t', 1, inplace=True)\n",
    "df_news['CATEGORY'].replace('e', 2, inplace=True)\n",
    "df_news['CATEGORY'].replace('m', 3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df_news, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_valid, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace=True)\n",
    "df_valid.reset_index(inplace=True)\n",
    "df_test.reset_index(inplace=True)\n",
    "df_train.drop('index', axis=1, inplace=True)\n",
    "df_valid.drop('index', axis=1, inplace=True)\n",
    "df_test.drop('index', axis=1, inplace=True)\n",
    "df_train.drop(436, axis=0, inplace=True)\n",
    "df_train.reset_index(inplace=True)\n",
    "df_train.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train = df_train['TITLE']\n",
    "y_train = df_train['CATEGORY']\n",
    "#y_train = torch.tensor(y_train).long()\n",
    "title_valid = df_valid['TITLE']\n",
    "y_valid = df_valid['CATEGORY']\n",
    "#y_valid = torch.tensor(y_valid).long()\n",
    "title_test = df_test['TITLE']\n",
    "y_test = df_test['CATEGORY']\n",
    "#y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/ryomisawa/Downloads/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300\n",
    "\n",
    "x_train = []\n",
    "\n",
    "for text in title_train:\n",
    "    text = text.split()\n",
    "    t = len(text)\n",
    "    x = 0\n",
    "    for word in text:\n",
    "        if word not in model:\n",
    "            x += np.zeros(d)\n",
    "        else:\n",
    "            x += model[word]\n",
    "    x = x/t\n",
    "    x_train.append(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = []\n",
    "\n",
    "for text in title_valid:\n",
    "    text = text.split()\n",
    "    t = len(text)\n",
    "    x = 0\n",
    "    for word in text:\n",
    "        if word not in model:\n",
    "            x += np.zeros(d)\n",
    "        else:\n",
    "            x += model[word]\n",
    "    x = x/t\n",
    "    x_valid.append(list(x))\n",
    "x_valid = torch.tensor(x_valid).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "\n",
    "for text in title_test:\n",
    "    text = text.split()\n",
    "    t = len(text)\n",
    "    x = 0\n",
    "    for word in text:\n",
    "        if word not in model:\n",
    "            x += np.zeros(d)\n",
    "        else:\n",
    "            x += model[word]\n",
    "    x = x/t\n",
    "    x_test.append(list(x))\n",
    "x_test = torch.tensor(x_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "W = torch.randn(d, L, requires_grad=True)\n",
    "x_1 = torch.tensor(x_train[0])\n",
    "x_1_4 = torch.tensor([x_train[0], x_train[1], x_train[2], x_train[3]])\n",
    "y_1 = f.softmax(torch.matmul(x_1, W))\n",
    "Y = f.softmax(torch.matmul(x_1_4, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_1 = -torch.log(y_1[y_train[0]])\n",
    "l_1_4 = 0\n",
    "for i in range(4):\n",
    "    l_1_4 += -torch.log(Y[i][y_train[i]])\n",
    "l_1_4 = l_1_4/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 3.0262e-03,  6.8466e-02, -7.2572e-02,  1.0798e-03],\n        [ 1.6893e-03,  3.8219e-02, -4.0511e-02,  6.0278e-04],\n        [-6.5253e-04, -1.4763e-02,  1.5648e-02, -2.3284e-04],\n        ...,\n        [ 4.5177e-04,  1.0221e-02, -1.0834e-02,  1.6121e-04],\n        [ 4.5096e-04,  1.0203e-02, -1.0815e-02,  1.6092e-04],\n        [-2.7411e-04, -6.2017e-03,  6.5736e-03, -9.7813e-05]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "l_1.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 2.5744e-02,  8.9736e-02, -1.1825e-01,  2.7675e-03],\n        [-6.8984e-03,  5.4763e-02, -5.0717e-02,  2.8516e-03],\n        [-1.3204e-03, -2.2393e-02,  2.3014e-02,  6.9902e-04],\n        ...,\n        [ 1.5911e-03,  7.7844e-03, -5.4977e-03, -3.8779e-03],\n        [-3.1871e-02,  1.9434e-02,  5.5613e-03,  6.8757e-03],\n        [ 1.3581e-02, -9.1696e-03, -4.2967e-03, -1.1477e-04]])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "l_1_4.backward()\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y_train).long()\n",
    "x_valid = torch.tensor(x_valid).float()\n",
    "y_valid = torch.tensor(y_valid).long()\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train).float()\n",
    "epoch = 101\n",
    "W = torch.randn(d, L, requires_grad=True)\n",
    "optimizer = optim.SGD([W], lr=0.1)\n",
    "for i in range(epoch):\n",
    "     y = f.softmax(torch.matmul(x_train, W))\n",
    "     l = 0\n",
    "     for j in range(len(x_train)):\n",
    "         l += -torch.log(y[j][y_train[j]])\n",
    "     l = l/len(x_train)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "     l.backward()\n",
    "\n",
    "     optimizer.step()\n",
    "     \n",
    "     if i % 10 == 0:\n",
    "         print(l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = f.softmax(torch.matmul(x_train, W))\n",
    "y_pred_test = f.softmax(torch.matmul(x_test, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Accuracy: 0.390, Test Accuracy: 0.373\n"
    }
   ],
   "source": [
    "print('Train Accuracy: {:.3f}, Test Accuracy: {:.3f}'.format(accuracy_score(y_train, y_pred_train.argmax(axis=1)), accuracy_score(y_test, y_pred_test.argmax(axis=1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "W = torch.randn(d, L, requires_grad=True)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "optimizer = optim.SGD([W], lr=0.1)\n",
    "for i in range(epoch):\n",
    "     pred_train = f.softmax(torch.matmul(x_train, W))\n",
    "     l_train = 0\n",
    "     for j in range(len(x_train)):\n",
    "         l_train += -torch.log(pred_train[j][y_train[j]])\n",
    "     l_train = l_train/len(x_train)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "     l_train.backward()\n",
    "\n",
    "     optimizer.step()\n",
    "\n",
    "     pred_valid = f.softmax(torch.matmul(x_valid, W))\n",
    "     l_valid = 0\n",
    "     for j in range(len(x_valid)):\n",
    "         l_valid += -torch.log(pred_valid[j][y_valid[j]])\n",
    "     l_valid = l_valid/len(x_valid)\n",
    "\n",
    "     y_pred_train = f.softmax(torch.matmul(x_train, W))\n",
    "     y_pred_valid = f.softmax(torch.matmul(x_valid, W))\n",
    "     \n",
    "     loss_train.append(l_train.item())\n",
    "     loss_valid.append(l_valid.item())\n",
    "     acc_train.append(accuracy_score(y_train, y_pred_train.argmax(axis=1)))\n",
    "     acc_valid.append(accuracy_score(y_valid, y_pred_valid.argmax(axis=1)))\n",
    "\n",
    "     x = [i for i in range(len(loss_train))]\n",
    "     plt.plot(x, loss_train)\n",
    "     plt.plot(x, loss_valid)\n",
    "     plt.plot(x, acc_train)\n",
    "     plt.plot(x, acc_valid)\n",
    "     plt.show()\n",
    "     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(d, L, requires_grad=True)\n",
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "optimizer = optim.SGD([W], lr=0.1)\n",
    "for i in range(10):\n",
    "     pred_train = f.softmax(torch.matmul(x_train, W))\n",
    "     l_train = 0\n",
    "     for j in range(len(x_train)):\n",
    "         l_train += -torch.log(pred_train[j][y_train[j]])\n",
    "     l_train = l_train/len(x_train)\n",
    "\n",
    "     optimizer.zero_grad()\n",
    "     l_train.backward()\n",
    "\n",
    "     optimizer.step()\n",
    "\n",
    "     pred_valid = f.softmax(torch.matmul(x_valid, W))\n",
    "     l_valid = 0\n",
    "     for j in range(len(x_valid)):\n",
    "         l_valid += -torch.log(pred_valid[j][y_valid[j]])\n",
    "     l_valid = l_valid/len(x_valid)\n",
    "\n",
    "     y_pred_train = f.softmax(torch.matmul(x_train, W))\n",
    "     y_pred_valid = f.softmax(torch.matmul(x_valid, W))\n",
    "     \n",
    "     loss_train.append(l_train.item())\n",
    "     loss_valid.append(l_valid.item())\n",
    "     acc_train.append(accuracy_score(y_train, y_pred_train.argmax(axis=1)))\n",
    "     acc_valid.append(accuracy_score(y_valid, y_pred_valid.argmax(axis=1)))\n",
    "\n",
    "     #x = [i for i in range(len(loss_train))]\n",
    "     #plt.plot(x, loss_train)\n",
    "     #plt.plot(x, loss_valid)\n",
    "     #plt.plot(x, acc_train)\n",
    "     #plt.plot(x, acc_valid)\n",
    "     #plt.show()\n",
    "\n",
    "     torch.save(\n",
    "         {\n",
    "             'epoch':i,\n",
    "             'weight':W,\n",
    "             'optimizer_state_dict':optimizer.state_dict()\n",
    "         },\n",
    "         'model{}.pth'.format(i)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "経過時間:1.20 訓練誤差:1.016 検証誤差:0.834\n経過時間:1.36 訓練誤差:1.069 検証誤差:0.883\n経過時間:1.35 訓練誤差:1.836 検証誤差:0.353\n経過時間:1.27 訓練誤差:0.277 検証誤差:0.493\n経過時間:1.10 訓練誤差:0.668 検証誤差:0.145\n経過時間:1.24 訓練誤差:1.009 検証誤差:0.296\n経過時間:1.23 訓練誤差:0.395 検証誤差:0.950\n経過時間:1.34 訓練誤差:0.512 検証誤差:0.257\n経過時間:1.21 訓練誤差:0.376 検証誤差:0.098\n経過時間:1.26 訓練誤差:0.094 検証誤差:0.213\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "W = torch.randn(d, L, requires_grad=True)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "optimizer = optim.SGD([W], lr=0.1)\n",
    "\n",
    "num_data_train = len(x_train)\n",
    "num_data_valid = len(x_valid)\n",
    "batch_size = 8\n",
    "\n",
    "for i in range(10):\n",
    "     t1 = time.time()\n",
    "     sff_idx_t = np.random.permutation(num_data_train)\n",
    "     sff_idx_v = np.random.permutation(num_data_valid)\n",
    "\n",
    "     for idx in range(0, num_data_train, batch_size):\n",
    "         \n",
    "         x_batch = x_train[sff_idx_t[idx: idx + batch_size if idx + batch_size < num_data_train else num_data_train]]\n",
    "         y_batch = y_train[sff_idx_t[idx: idx + batch_size if idx + batch_size < num_data_train else num_data_train]]\n",
    "         pred_train = f.softmax(torch.matmul(x_batch, W))\n",
    "         \n",
    "         l_train = 0\n",
    "         for i in range(len(y_batch)):\n",
    "             l_train += -torch.log(pred_train[i][y_batch[i]])\n",
    "         l_train = l_train/len(x_batch)\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         l_train.backward()\n",
    "\n",
    "         optimizer.step()\n",
    "     \n",
    "     for idx in range(0, num_data_valid, batch_size):\n",
    "         x_batch = x_valid[sff_idx_v[idx: idx + batch_size if idx + batch_size < num_data_valid else num_data_valid]]\n",
    "         y_batch = y_valid[sff_idx_v[idx: idx + batch_size if idx + batch_size < num_data_valid else num_data_valid]]\n",
    "         pred_valid = f.softmax(torch.matmul(x_batch, W))\n",
    "\n",
    "         l_valid = 0\n",
    "         for i in range(len(y_batch)):\n",
    "             l_valid += -torch.log(pred_valid[i][y_batch[i]])\n",
    "         l_valid = l_valid/len(x_batch)\n",
    "         y_pred_valid = f.softmax(torch.matmul(x_batch, W))\n",
    "     t2 = time.time()\n",
    "     elapsed_time = t2 - t1\n",
    "     print(f'経過時間:{elapsed_time:.2f}', f'訓練誤差:{l_train.item():.3f}', f'検証誤差:{l_valid.item():.3f}')\n",
    "\n",
    "     if i == 9:\n",
    "         times.append(elapsed_time)\n",
    "     #loss_train.append(l_train.item())\n",
    "     #loss_valid.append(l_valid.item())\n",
    "     #acc_train.append(accuracy_score(y_train, y_pred_train.argmax(axis=1)))\n",
    "     #acc_valid.append(accuracy_score(y_valid, y_pred_valid.argmax(axis=1)))\n",
    "\n",
    "     #x = [i for i in range(len(loss_train))]\n",
    "     #plt.plot(x, loss_train)\n",
    "     #plt.plot(x, loss_valid)\n",
    "     #plt.plot(x, acc_train)\n",
    "     #plt.plot(x, acc_valid)\n",
    "     #plt.show()\n",
    "\n",
    "     torch.save(\n",
    "         {\n",
    "             'epoch':i,\n",
    "             'weight':W,\n",
    "             'optimizer_state_dict':optimizer.state_dict()\n",
    "         },\n",
    "         'model{}.pth'.format(i)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "経過時間:1.1476891040802002 0.9563228543596468 0.698005071211004\n経過時間:0.9534740447998047 0.6148715959875838 0.57455320482304\n経過時間:0.9485177993774414 0.5243028376580983 0.5193493579051451\n経過時間:0.9258489608764648 0.47535558734675576 0.4872455317192449\n経過時間:0.92983078956604 0.44382499426329647 0.4659205999916899\n経過時間:0.9367928504943848 0.4218704569330816 0.4507430860003133\n経過時間:0.9438028335571289 0.4052458920151643 0.4413008286790577\n経過時間:0.9247448444366455 0.39218815036656995 0.4321495979176667\n経過時間:0.9181609153747559 0.3816246898102014 0.4236053497290718\n経過時間:0.9252920150756836 0.3729197781196498 0.41832490665782357\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "W = torch.randn(d, L, requires_grad=True)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "optimizer = optim.SGD([W], lr=0.1)\n",
    "\n",
    "num_data_train = len(x_train)\n",
    "num_data_valid = len(x_valid)\n",
    "batch_size = 8\n",
    "\n",
    "device = torch.device('cude' if torch.cuda.is_available() else 'cpu')\n",
    "for i in range(10):\n",
    "     t1 = time.time()\n",
    "     sff_idx_t = np.random.permutation(num_data_train)\n",
    "     sff_idx_v = np.random.permutation(num_data_valid)\n",
    "\n",
    "     loss_train = []\n",
    "     loss_valid = []\n",
    "     for idx in range(0, num_data_train, batch_size):\n",
    "         \n",
    "         x_batch = x_train[sff_idx_t[idx: idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "         y_batch = y_train[sff_idx_t[idx: idx + batch_size if idx + batch_size < num_data_train else num_data_train]].to(device)\n",
    "\n",
    "         pred_train = f.softmax(torch.matmul(x_batch, W))\n",
    "         \n",
    "         l_train = 0\n",
    "         for i in range(len(y_batch)):\n",
    "             l_train += -torch.log(pred_train[i][y_batch[i]])\n",
    "         l_train = l_train/len(x_batch)\n",
    "\n",
    "         optimizer.zero_grad()\n",
    "         l_train.backward()\n",
    "\n",
    "         optimizer.step()\n",
    "\n",
    "         loss_train.append(l_train.item())\n",
    "    \n",
    "     \n",
    "     for idx in range(0, num_data_valid, batch_size):\n",
    "         x_batch = x_valid[sff_idx_v[idx: idx + batch_size if idx + batch_size < num_data_valid else num_data_valid]]\n",
    "         y_batch = y_valid[sff_idx_v[idx: idx + batch_size if idx + batch_size < num_data_valid else num_data_valid]]\n",
    "\n",
    "         x_batch = x_batch.to(device)\n",
    "         y_batch = y_batch.to(device)\n",
    "         \n",
    "         pred_valid = f.softmax(torch.matmul(x_batch, W))\n",
    "         \n",
    "\n",
    "         l_valid = 0\n",
    "         for i in range(len(y_batch)):\n",
    "             l_valid += -torch.log(pred_valid[i][y_batch[i]])\n",
    "         l_valid = l_valid/len(x_batch)\n",
    "         y_pred_valid = f.softmax(torch.matmul(x_batch, W))\n",
    "\n",
    "         loss_valid.append(l_valid.item())\n",
    "\n",
    "     t2 = time.time()\n",
    "     elapsed_time = t2 - t1\n",
    "     print(f'経過時間:{elapsed_time}', np.mean(loss_train), np.mean(loss_valid))\n",
    "\n",
    "     if i == 9:\n",
    "         times.append(elapsed_time)\n",
    "     #loss_train.append(l_train.item())\n",
    "     #loss_valid.append(l_valid.item())\n",
    "     #acc_train.append(accuracy_score(y_train, y_pred_train.argmax(axis=1)))\n",
    "     #acc_valid.append(accuracy_score(y_valid, y_pred_valid.argmax(axis=1)))\n",
    "\n",
    "     #x = [i for i in range(len(loss_train))]\n",
    "     #plt.plot(x, loss_train)\n",
    "     #plt.plot(x, loss_valid)\n",
    "     #plt.plot(x, acc_train)\n",
    "     #plt.plot(x, acc_valid)\n",
    "     #plt.show()\n",
    "\n",
    "     torch.save(\n",
    "         {\n",
    "             'epoch':i,\n",
    "             'weight':W,\n",
    "             'optimizer_state_dict':optimizer.state_dict()\n",
    "         },\n",
    "         'model{}.pth'.format(i)\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600071158503",
   "display_name": "Python 3.7.7 64-bit ('nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}